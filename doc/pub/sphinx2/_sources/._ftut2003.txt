.. !split

.. _ftut:nonlinear:

Implementing solvers for nonlinear PDEs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


..

    FEniCS almost automates the solution of nonlinear problems, as shown
    in the
    section "A nonlinear Poisson equation": "" in [Ref1]_.
    However, in problems with severe nonlinearities it can be hard to
    obtain convergence of the iterative methods, and one needs to
    tailor the solution to the problem at hand. FEniCS is very well suited
    for this, but then the nonlinear iteration algorithm must be hand-coded.
    This chapter shows the implementation of three basic solution strategies:
    
      * a simple Picard-type iteration
    
      * a Newton method at the algebraic level
    
      * a Newton method at the PDE level
    
    The detailed exposition will hopefully also
    help newcomers to the finite element method to understand the method
    better.



**Test problem.**
As basic test problem in this chapter, we use the nonlinear Poisson problem

.. math::
         -\nabla\cdot(q(u)\nabla u)=f{\thinspace .}

It turns out that

.. _Eq:_auto12:

.. math::

    \tag{20}
    u(x_0,\ldots,x_{d-1}) = \left((2^{m+1}-1)x_0 + 1\right)^{1/(m+1)} - 1
        
        

is an exact solution of this nonlinear PDE if :math:`f=0`, :math:`q(u)=(1+u)^m`,
the domain :math:`\Omega` is the unit hypercube :math:`[0, 1]^d` in
:math:`d` dimensions, :math:`u=0` for :math:`x_0=0`, :math:`u=1` for
:math:`x_0=1`, and :math:`\partial u/\partial n=0` at all other boundaries :math:`x_i=0`
and :math:`x_i=1`, :math:`i=1,\ldots,d-1`. The coordinates are now represented by
the symbols :math:`x_0,\ldots,x_{d-1}`.

We refer to the
section "A nonlinear Poisson equation": "" in [Ref1]_
for details on formulating a PDE
problem in :math:`d` space dimensions.

.. _ch:poisson0:nonlinear:

The built-in automated Newton solver
====================================

Solving nonlinear problems in FEniCS is a matter of defining the
nonlinear variational form ``F`` and calling ``solve(F == 0, ...)`` as
explained in 
the section "A simple FEniCS implementation": "" in [Ref1]_.
However, in real problems we will at least have control of the
tolerance used in the stopping criterion in Newton's method.
Sometimes we also want to derive the Jacobian ourselves or
provide approximations. Such customizations are shown next.

Computing the Jacobian
----------------------

.. index:: ft13_nlpoisson_func.py

.. index:: nonlinear variational problems

.. index:: NonlinearVariationalProblem

.. index:: NonlinearVariationalSolver

We may provide the Jacobian for Newton's method as argument to ``solve``:
``solve(F == 0, u, bcs=bcs, J=J)``. However, if you have several
nonlinear problems within the same code and want different solvers
for them, it is better to use these objects:

.. code-block:: python

    problem = NonlinearVariationalProblem(F, u, bcs, J)
    solver  = NonlinearVariationalSolver(problem)
    solver.solve()

Here, ``F`` corresponds to the nonlinear form :math:`F(u;v)`,
``u`` is the unknown ``Function`` object, ``bcs``
represents the essential boundary conditions (in general a list of
``DirichletBC`` objects), and
``J`` is a variational form for the Jacobian of ``F``.

The ``F`` form corresponding to

.. _Eq:_auto13:

.. math::

    \tag{21}
    F(u; v) = \int_\Omega q(u)\nabla u\cdot \nabla v + fv {\, \mathrm{d}x},
        
        

is straightforwardly defined as follows, assuming ``q(u)`` is
coded as a Python function (see

the section "A simple FEniCS implementation": "" in [Ref1]_) for
code):

.. code-block:: python

    u_ = Function(V)     # most recently computed solution
    v  = TestFunction(V)
    F  = dot(q(u_)*grad(u_), grad(v))*dx

Note here that ``u_`` is a ``Function`` and not a ``TrialFunction``.
An alternative and perhaps more intuitive formula for :math:`F` is to
define :math:`F(u;v)` directly in terms of
a trial function for :math:`u` and a test function for :math:`v`, and then
create the proper ``F`` by

.. code-block:: python

    u  = TrialFunction(V)
    v  = TestFunction(V)
    F  = dot(q(u)*grad(u), grad(v))*dx
    u_ = Function(V)     # the most recently computed solution
    F  = action(F, u_)

The latter statement is equivalent to :math:`F(u=u^{-}; v)`, where :math:`u^{-}` is
an existing finite element function representing the most recently
computed approximation to the solution.
(Note that :math:`u^k` and :math:`u^{k+1}` in the previous notation
correspond to :math:`u^{-}` and :math:`u` in the present
notation. We have changed notation to better align the mathematics with
the associated code.)

.. index:: Gateaux derivative

The derivative :math:`J` (``J``) of :math:`F` (``F``) is formally the Gateaux
derivative :math:`DF(u^k; \delta u, v)` of :math:`F(u;v)` at :math:`u=u^{-}` in the
direction of :math:`\delta u`.  Technically, this Gateaux derivative is
derived by computing

.. _Eq:ch:poisson0:nonlinear:Gateaux1:

.. math::

    \tag{22}
    \lim_{\epsilon\rightarrow 0}{d\over d\epsilon} F_i(u^{-} + \epsilon\delta u; v)
        {\thinspace .}  
        

The :math:`\delta u` is now the trial function and :math:`u^{-}` is the previous
approximation to the solution :math:`u`.  We start with

.. math::
        
        {d\over d\epsilon}\int_\Omega \nabla v\cdot\left( q(u^{-} + \epsilon\delta u)
        \nabla (u^{-} + \epsilon\delta u)\right) {\, \mathrm{d}x}
        

and obtain

.. math::
        
        \int_\Omega \nabla v\cdot\left\lbrack
        q'(u^{-} + \epsilon\delta u)\delta u
        \nabla (u^{-} + \epsilon\delta u)
        +
        q(u^{-} + \epsilon\delta u)
        \nabla \delta u
        \right\rbrack {\, \mathrm{d}x},
        

which leads to

.. _Eq:_auto14:

.. math::

    \tag{23}
    \int_\Omega \nabla v\cdot\left\lbrack
        q'(u^{-})\delta u
        \nabla (u^{-})
        +
        q(u^{-})
        \nabla \delta u
        \right\rbrack {\, \mathrm{d}x},
        
        

as :math:`\epsilon\rightarrow 0`.  This last expression is the Gateaux
derivative of :math:`F`. We may use :math:`J` or :math:`a(\delta u, v)` for this
derivative. The latter has the advantage of being easy to recognize
as a bilinear form. However, in the forthcoming code
examples ``J`` is used as variable name for the Jacobian.

The specification of ``J`` goes as follows if ``du`` is the
``TrialFunction``:

.. code-block:: python

    du = TrialFunction(V)
    v  = TestFunction(V)
    u_ = Function(V)      # the most recently computed solution
    F  = dot(q(u_)*grad(u_), grad(v))*dx
    
    J = dot(q(u_)*grad(du), grad(v))*dx + \ 
        dot(Dq(u_)*du*grad(u_), grad(v))*dx

With the alternative specification of ``F``, where ``u`` is a
``TrialFunction``, ``J`` is computed by

.. code-block:: python

    u  = TrialFunction(V)
    v  = TestFunction(V)
    u_ = Function(V)      # the most recently computed solution
    F  = dot(q(u)*grad(u), grad(v))*dx
    F  = action(F, u_)
    
    J = dot(q(u_)*grad(u), grad(v))*dx + \ 
        dot(Dq(u_)*u*grad(u_), grad(v))*dx

.. index:: derivative

.. index:: automatic differentiation

.. index::
   single: Jacobian, automatic computation

The UFL language, used to specify weak forms, supports differentiation
of forms. This feature facilitates automatic *symbolic* computation of the
Jacobian ``J`` by calling the function ``derivative`` with ``F``, the most
recently computed solution (``Function``), and the unknown
(``TrialFunction``) as parameters:

.. code-block:: python

    du = TrialFunction(V)
    v  = TestFunction(V)
    u_ = Function(V)      # the most recently computed solution
    F  = dot(q(u_)*grad(u_), grad(v))*dx
    
    J  = derivative(F, u_, du)  # Gateaux derivative in dir. of du

or

.. code-block:: python

    u  = TrialFunction(V)
    v  = TestFunction(V)
    u_ = Function(V)      # the most recently computed solution
    F  = dot(q(u)*grad(u), grad(v))*dx
    F  = action(F, u_)
    
    J  = derivative(F, u_, u)   # Gateaux derivative in dir. of u

The ``derivative`` function is obviously very convenient in problems
where differentiating ``F`` by hand implies lengthy calculations.

.. For difference between diff and derivative, see p 531 in the FEniCS book.

.. Hyperelasticity is an application that contains both functions.

Setting solver parameters
-------------------------

The following code defines the nonlinear variational problem and
an associated solver based on Newton's method. We here demonstrate
how key parameters in
Newton's method can be set, as well as the choice of
solver and preconditioner, and associated parameters, for the
linear system occurring in the Newton iterations.

.. code-block:: python

    problem = NonlinearVariationalProblem(F, u_, bcs, J)
    solver  = NonlinearVariationalSolver(problem)
    prm = solver.parameters
    prm_n = prm['newton_solver']
    prm_n['absolute_tolerance'] = abs_tol_Newton
    prm_n['relative_tolerance'] = rel_tol_Newton
    prm_n['maximum_iterations'] = max_iter_Newton
    prm_n['relaxation_parameter'] = relaxation_prm_Newton
    if linear_solver == 'Krylov':
        prec = 'jacobi' if 'jacobi' in \ 
               list(zip(*krylov_solver_preconditioners()))[0] \ 
               else 'ilu'
        prm_n['linear_solver'] = 'gmres'
        prm_n['preconditioner'] = prec
        prm_nk = prm_n['krylov_solver']
        prm_nk['absolute_tolerance'] = abs_tol_Krylov
        prm_nk['relative_tolerance'] = rel_tol_Krylov
        prm_nk['maximum_iterations'] = max_iter_Krylov
        prm_nk['monitor_convergence'] = True
        prm_nk['nonzero_initial_guess'] = False
        prm_nk['gmres']['restart'] = 40
        prm_nk['preconditioner']['structure'] = \ 
                                        'same_nonzero_pattern'
        prm_nk['preconditioner']['ilu']['fill_level'] = 0

A list of available parameters and their default values can as
usual be printed by calling ``info(prm, True)``.
The ``u_`` we feed to the nonlinear variational problem object
is filled with the solution by the call ``solver.solve()``.

FEniCS implementation          (3)
----------------------------------

The preferred implementation of ``F`` and ``J``, depending on whether ``du``
or ``u`` is the ``TrialFunction`` object, is a matter of personal
taste. Derivation of the Gateaux derivative by hand, as shown above,
is most naturally matched by an implementation where ``du`` is the
``TrialFunction``, while use of automatic symbolic differentiation with
the aid of the ``derivative`` function is most naturally matched by an
implementation where ``u`` is the ``TrialFunction``.  We have implemented
both approaches in a ``solver`` function in
the file `ft13_nlpoisson_func.py <https://github.com/hplgit/fenics-tutorial/blob/master/src/ft13_nlpoisson_func.py>`__.
An argument ``TrialFunction_object``
can be set to ``u`` if we want to have ``u`` as
``TrialFunction``, otherwise it set to ``du``.

.. code-block:: python

    def solver(
        q, Dq, f, divisions, degree=1,
        method='u', J_comp='manual',
        linear_solver='Krylov', # Alt: 'direct'
        abs_tol_Krylov=1E-5,
        rel_tol_Krylov=1E-5,
        abs_tol_Newton=1E-5,
        rel_tol_Newton=1E-5,
        max_iter_Krylov=1000,
        max_iter_Newton=50,
        relaxation_prm_Newton=1.0,
        log_level=PROGRESS,
        dump_parameters=False,
        ):

Let us run the code for :math:`m=2` and three meshes corresponding to
``divisions`` as ``(10, 10)``, ``(20, 20)``, and ``(40, 40)``. The maximum
errors are then :math:`5\cdot 10^{-3}`, :math:`1.7\cdot 10^{-3}\approx
\frac{1}{4}5\cdot 10^{-3}`, :math:`4.5\cdot
10^{-4}\approx\frac{1}{4}1.7\cdot 10^{-3}`, and :math:`1.2\cdot
10^{-4}\approx\frac{1}{4}4.5\cdot 10^{-4}`, demonstrating second-order
convergence in the cell size as expected for P1 elements.  This test
is turned into a proper unit test in the ``test_solver`` function.

Manual implementation of solution algorithms
============================================

We now show how we can implement the solution algorithm for
handling nonlinear PDEs from scratch. First, we treat the
common and popular Picard iteration method. Second, we look at
a standard Newton method for solving nonlinear algebraic equations.
And third, we demonstrate application of the Newton method directly
to the PDE, which is an approach that is more attractive to
FEniCS programmers than the second one.

.. _ftut:nonlinear:Picard:

Picard iteration
----------------

.. index:: Picard iteration

.. index:: successive substitutions

Idea
~~~~

Picard iteration, also called the method of successive substitutions,
is an easy way of handling nonlinear PDEs: we simply
use a known, previous solution in the nonlinear terms so that these
terms become linear in the unknown :math:`u`. The strategy is also known as
the method of successive substitutions.  For our particular problem,
we use a known, previous solution in the coefficient :math:`q(u)`.  More
precisely, given a solution :math:`u^k` from iteration :math:`k`, we seek a new
(hopefully improved) solution :math:`u^{k+1}` in iteration :math:`k+1` such that
:math:`u^{k+1}` solves the *linear problem*,

.. _Eq:ch:poisson0:nonlinear:picard1:

.. math::

    \tag{24}
    \nabla\cdot \left(q(u^k)\nabla u^{k+1}\right) = 0,\quad k=0,1,\ldots
        

The iterations require an initial guess :math:`u^0`.
The hope is that :math:`u^{k} \rightarrow u` as :math:`k\rightarrow\infty`, and that
:math:`u^{k+1}` is sufficiently close to the exact
solution :math:`u` of the discrete problem after just a few iterations.

A similar linearization is needed in Neumann and Robin conditions as
well, e.g., the condition

.. math::
         -q(u)\frac{\partial u}{\partial n} = r(u-U_s)

is linearized to

.. math::
         -q(u^k)\frac{\partial u^{k+1}}{\partial n} = r(u^{k+1}-U_s){\thinspace .}

Variational formulation
~~~~~~~~~~~~~~~~~~~~~~~

We can easily formulate a variational problem for :math:`u^{k+1}` from
:ref:`(24) <Eq:ch:poisson0:nonlinear:picard1>` by the same steps as for a
linear Poisson problem.
The problem boils down to seeking
:math:`u^{k+1} \in V` such that

.. _Eq:ch:poisson0:nonlinear:picard2:

.. math::

    \tag{25}
    \tilde F(u^{k+1}; v) = 0 \quad \forall v \in \hat{V},\quad k=0,1,\ldots,
        

with

.. _Eq:ch:poisson0:nonlinear:picard3:

.. math::

    \tag{26}
    \tilde F(u^{k+1}; v) = \int_\Omega q(u^k)\nabla u^{k+1}\cdot \nabla v {\, \mathrm{d}x}
        {\thinspace .}
        

Since this is a linear problem in the unknown :math:`u^{k+1}`, we can equivalently
use the formulation

.. _Eq:_auto15:

.. math::

    \tag{27}
    a(u^{k+1},v) = L(v),
        
        

with :math:`a(u,v) = \int_\Omega q(u^k)\nabla u\cdot \nabla v {\, \mathrm{d}x}` and
:math:`L(v) = 0`.

Stopping criteria
~~~~~~~~~~~~~~~~~

The iterations can be stopped when :math:`\epsilon\equiv ||u^{k+1}-u^k|| <
\mbox{tol}`, where :math:`\mbox{tol}` is a small tolerance, say :math:`10^{-5}`,
or when the number of iterations exceed some critical limit. The
latter case will pick up divergence of the method or unacceptable slow
convergence. The residual
criterion :math:`||\epsilon_F \equiv ||F(u^{k+1};v)|| < \mbox{tol}`
can also be used, but it would require
a kind of extra iteration since we must compute the linear system in
the next iteration to be in position to evaluate :math:`||F(u^{k+1};v)||`.
However, since the residual criterion is costly, we use the change
in solution :math:`\epsilon` instead.

.. index:: ft14_nlpoisson_picard.py

FEniCS implementation          (4)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the solution algorithm we only need to store :math:`u^k` and :math:`u^{k+1}`,
called ``u_`` and ``u`` in the code below (our convention is that
``u`` is always the unknown to be computed in a code, and ``u_`` is
the most recently computed approximation to the mathematical solution
of the problem).
The algorithm can then be expressed as follows:

.. code-block:: python

    def q(u):
        return (1+u)**m
    
    # Define variational problem for Picard iteration
    u = TrialFunction(V)
    v = TestFunction(V)
    u_ = interpolate(Constant(0.0), V)  # previous (known) u
    a = dot(q(u_)*grad(u), grad(v))*dx
    f = Constant(0.0)
    L = f*v*dx
    
    # Picard iterations
    u = Function(V)     # new unknown function
    eps = 1.0           # error measure ||u-u_||
    tol = 1.0E-5        # tolerance
    iter = 0            # iteration counter
    maxiter = 25        # max no of iterations allowed
    while eps > tol and iter < maxiter:
        iter += 1
        solve(a == L, u, bcs)
        du = u.vector().array() - u_.vector().array()
        eps = np.linalg.norm(du, ord=numpy.Inf)
        print('iter=%d: norm=%g' % (iter, eps))
        u_.assign(u)   # update for next iteration

We need to define the previous solution in the iterations, ``u_``,
as a finite element function so that ``u_`` can be updated with
``u`` at the end of the loop. We may create the initial
``Function u_``
by interpolating
an ``Expression`` or a ``Constant``
to the same vector space as ``u`` lives in (``V``).

In the code above we demonstrate how to use
``numpy`` functionality to compute the norm of
the difference between the two most recent solutions. Here we apply
the maximum norm (:math:`\ell_\infty` norm) on the difference of the solution vectors
(``ord=1`` and ``ord=2`` give the :math:`\ell_1` and :math:`\ell_2` vector
norms - other norms are possible for ``numpy`` arrays,
see ``pydoc numpy.linalg.norm``).

The file ``ft14_nlpoisson_picard.py`` contains the complete
code for this nonlinear Poisson problem.  The implementation is :math:`d`
dimensional, with mesh construction and setting of Dirichlet
conditions as explained in the section :ref:`ch:poisson0:nD`.  For a
:math:`33\times 33` grid with :math:`m=2` we need 9 iterations for convergence
when the tolerance is :math:`10^{-5}`.

.. _ftut:nonlinear:Newton:algebraic:

A Newton method at the algebraic level
--------------------------------------

.. index:: Newton's method (algebraic equations)

Identifying the variational formulation of the linearized problem
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

After having discretized our nonlinear PDE problem, we may
use Newton's method to solve the system of nonlinear algebraic equations.
From the variational problem :math:`F(u; v) = 0\ \forall v\in\hat{V}`,
we get a system of equations for the unknown parameters :math:`U_1,\ldots, U_N`
(by inserting :math:`u = \sum_{j=1}^N U_j \phi_j`
and :math:`v=\hat\phi_i` in :math:`F(u;v)`):

.. _Eq:ftut:nonlinear:Newton:F1:

.. math::

    \tag{28}
    F_i(U_1,\ldots,U_N) \equiv
        \sum_{j=1}^N
        \int_\Omega \left( q\left(\sum_{\ell=1}^NU_\ell\phi_\ell\right)
        \nabla \phi_j U_j\right)\cdot \nabla \hat\phi_i {\, \mathrm{d}x} = 0,\quad i=1,\ldots,N{\thinspace .}
        
        

Newton's method for the system :math:`F_i(U_1,\ldots,U_j)=0`, :math:`i=1,\ldots,N`
can be formulated as

.. _Eq:_auto16:

.. math::

    \tag{29}
    \sum_{j=1}^N
        {\partial \over\partial U_j} F_i(U_1^k,\ldots,U_N^k)\delta U_j
        = -F_i(U_1^k,\ldots,U_N^k),\quad i=1,\ldots,N,
        
        

.. _Eq:_auto17:

.. math::

    \tag{30}
    U_j^{k+1} = U_j^k + \omega\delta U_j,\quad j=1,\ldots,N,
        
        

where :math:`\omega\in [0,1]` is a relaxation parameter, and :math:`k` is
an iteration index. An initial guess :math:`u^0` must
be provided to start the algorithm.

.. index:: under-relaxation

The original Newton method has :math:`\omega=1`, but in problems where it is
difficult to obtain convergence,
so-called *under-relaxation* with :math:`\omega < 1` may help. It means that
one takes a smaller step than what is suggested by Newton's method.

.. index::
   single: Jacobian, manual computation

We need, in a program, to compute the Jacobian
matrix :math:`\partial F_i/\partial U_j`
and the right-hand side vector :math:`-F_i`.
Our present problem has :math:`F_i` given by :ref:`(28) <Eq:ftut:nonlinear:Newton:F1>`.
The derivative :math:`\partial F_i/\partial U_j` becomes

.. _Eq:ch:poisson0:nonlinear:dFdU:

.. math::

    \tag{31}
    \int\limits_\Omega \left\lbrack
         q'(\sum_{\ell=1}^NU_\ell^k\phi_\ell)\phi_j
        \nabla (\sum_{j=1}^NU_j^k\phi_j)\cdot \nabla \hat\phi_i
        +
        q\left(\sum_{\ell=1}^NU_\ell^k\phi_\ell\right)
        \nabla \phi_j \cdot \nabla \hat\phi_i
        \right\rbrack
         {\, \mathrm{d}x}{\thinspace .}
        
        

The following results were used to obtain :ref:`(31) <Eq:ch:poisson0:nonlinear:dFdU>`:

.. _Eq:_auto18:

.. math::

    \tag{32}
    {\partial u\over\partial U_j} = {\partial\over\partial U_j}
        \sum_{j=1}^NU_j\phi_j = \phi_j,\quad {\partial\over\partial U_j}\nabla u = \nabla\phi_j,\quad {\partial\over\partial U_j}q(u) = q'(u)\phi_j{\thinspace .}
        
        

We can reformulate the Jacobian matrix
in :ref:`(31) <Eq:ch:poisson0:nonlinear:dFdU>`
by introducing the short
notation :math:`u^k = \sum_{j=1}^NU_j^k\phi_j`:

.. _Eq:_auto19:

.. math::

    \tag{33}
    {\partial F_i\over\partial U_j} =
        \int_\Omega \left\lbrack
        q'(u^k)\phi_j
        \nabla u^k \cdot \nabla \hat\phi_i
        +
        q(u^k)
        \nabla \phi_j \cdot \nabla \hat\phi_i
        \right\rbrack {\, \mathrm{d}x}{\thinspace .}
        
        

Now it is time to write such a key expression with the
computer code friendly notation :math:`u^{-}` for :math:`u^k`:

.. _Eq:_auto20:

.. math::

    \tag{34}
    {\partial F_i\over\partial U_j} =
        \int_\Omega \left\lbrack
        q'(u^{-})\phi_j
        \nabla u^{-} \cdot \nabla \hat\phi_i
        +
        q(u^{-})
        \nabla \phi_j \cdot \nabla \hat\phi_i
        \right\rbrack {\, \mathrm{d}x}{\thinspace .}
        
        

In order to make FEniCS compute this matrix, we need to formulate a
corresponding variational problem. Looking at the
linear system of equations in Newton's method,

.. math::
        
        \sum_{j=1}^N {\partial F_i\over\partial U_j}\delta U_j = -F_i,\quad
        i=1,\ldots,N,
        

we can introduce :math:`v` as a general test function replacing :math:`\hat\phi_i`,
and we can identify the unknown
:math:`\delta u = \sum_{j=1}^N\delta U_j\phi_j`. From the linear system
we can now go "backwards" to construct the corresponding linear
discrete weak form :math:`F(u;v)=0` to be solved in each Newton iteration:

.. _Eq:ftut:nonlinear:Newton:aLF:

.. math::

    \tag{35}
    F = \int_\Omega \left\lbrack
        q'(u^{-})\delta u
        \nabla u^{-} \cdot \nabla v
        +
        q(u^{-})
        \nabla \delta u\cdot \nabla v
        + q(u^{-})
        \nabla u^{-}\cdot \nabla v \right\rbrack {\, \mathrm{d}x}{\thinspace .}
        

Note the important feature in Newton's method
that the
previous solution :math:`u^{-}` replaces :math:`u`
in the formulas when computing the matrix
:math:`\partial F_i/\partial U_j` and vector :math:`F_i` for the linear system in
each Newton iteration.

.. index:: ft15_nlpoisson_alg_newton.py

FEniCS implementation          (5)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To obtain a good initial guess :math:`u^0`, we can solve a simplified, linear
problem, typically with :math:`q(u)=1`, which yields the standard Laplace
equation :math:`\nabla^2 u^0 =0`.
The code for computing the initial guess :math:`u^0` becomes as follows:

.. code-block:: python

    tol = 1E-14
    def left_boundary(x, on_boundary):
        return on_boundary and abs(x[0]) < tol
    
    def right_boundary(x, on_boundary):
        return on_boundary and abs(x[0]-1) < tol
    
    Gamma_0 = DirichletBC(V, Constant(0.0), left_boundary)
    Gamma_1 = DirichletBC(V, Constant(1.0), right_boundary)
    bcs = [Gamma_0, Gamma_1]
    
    # Define variational problem for initial guess (q(u)=1, i.e., m=0)
    u = TrialFunction(V)
    v = TestFunction(V)
    F = dot(grad(u), grad(v))*dx + Constant(0)*v*dx
    A, b = assemble_system(lhs(F), rhs(F), bcs)
    u_ = Function(V)
    U_ = u_.vector()
    solve(A, U_, b)

Here, ``u_`` is the initial guess we denote by :math:`u^0` in the mathematics.

The Dirichlet boundary conditions for :math:`\delta u`, in the problem to be
solved in each Newton iteration, are somewhat different than the
conditions for :math:`u`.  Assuming that :math:`u^k` fulfills the Dirichlet
conditions for :math:`u`, :math:`\delta u` must be zero at the boundaries where
the Dirichlet conditions apply, in order for :math:`u^{k+1}=u^k +
\omega\delta u` to fulfill the right boundary values. We therefore
define an additional list of Dirichlet boundary conditions objects for
:math:`\delta u`:

.. code-block:: python

    Gamma_0_du = DirichletBC(V, Constant(0), left_boundary)
    Gamma_1_du = DirichletBC(V, Constant(0), right_boundary)
    bcs_du = [Gamma_0_du, Gamma_1_du]

The nonlinear coefficient and its derivative must be defined
before coding the weak form of the Newton system:

.. code-block:: python

    def q(u):
        return (1+u)**m
    
    def Dq(u):
        return m*(1+u)**(m-1)
    
    # Define variational problem in a Newton iteration
    du = TrialFunction(V) # u = u_ + omega*du
    F = dot(q(u_)*grad(du), grad(v))*dx + \ 
        dot(Dq(u_)*du*grad(u_), grad(v))*dx + \ 
        dot(q(u_)*grad(u_), grad(v))*dx
    
    # Newton iteration at the algebraic level

The Newton iteration loop is very similar to the Picard iteration loop
in the section :ref:`ftut:nonlinear:Picard`:

.. code-block:: python

    du = Function(V)
    u  = Function(V)  # u = u_ + omega*du
    omega = 1.0       # relaxation parameter
    eps = 1.0
    tol = 1.0E-5
    num_iter = 0
    max_iter = 25
    # u_ must have right boundary conditions
    while eps > tol and iter < max_iter:
        num_iter += 1
        print(num_iter, 'iteration', end=' ')
        A, b = assemble_system(lhs(F), rhs(F), bcs_du)
        solve(A, du.vector(), b)
        eps = numpy.linalg.norm(du.vector().array(), ord=numpy.Inf)
        print('Norm:', eps)
        u.vector()[:] = u_.vector() + omega*du.vector()
        u_.assign(u)

There are other ways of implementing the
update of the solution as well:

.. code-block:: python

    u.assign(u_)  # u = u_
    u.vector().axpy(omega, du.vector())
    
    # or
    u.vector()[:] += omega*du.vector()

The ``axpy(a, y)`` operation adds a scalar ``a`` times a ``Vector``
``y`` to a ``Vector`` object.  It is usually a fast operation
calling up an optimized BLAS routine for the calculation.

The important message is that we cannot do (the slightly more
intuitive) ``u = u_ + omega*du`` because this is computed as an
UFL expression of a ``Function`` plus a scalar times a ``Function``.
We would need to project or interpolate this UFL expression ``u``
in :math:`V` in that case. Instead, we update the values through the
degrees of freedom vectors of the finite element function objects.

Mesh construction for a :math:`d`-dimensional problem with arbitrary degree of
the Lagrange elements can be done as
explained in the
section "Parameterizing the number of space dimensions": "" in
[Ref1]_.
The complete program appears in the file ``ft15_nlpoisson_alg_newton.py``.

.. _ftut:nonlinear:Newton:pdelevel:

A Newton method at the PDE level
--------------------------------

.. index:: Newton's method (PDE level)

Although Newton's method in PDE problems is normally formulated at the
linear algebra level, i.e., as a solution method for systems of nonlinear
algebraic equations, we can also formulate the method at the PDE level.
This approach yields a linearization of the PDEs before they are discretized.
FEniCS users will probably find this technique simpler to apply than
the more standard method in the section :ref:`ftut:nonlinear:Newton:algebraic`,
which needs an unfamiliar interpretation of a linear system as a
variational formulation.

The mathematical method
~~~~~~~~~~~~~~~~~~~~~~~

Given an approximation to the solution field, :math:`u^k`, we seek a
perturbation :math:`\delta u` so that

.. _Eq:_auto21:

.. math::

    \tag{36}
    u^{k+1} = u^k + \delta u
        
        

fulfills the nonlinear PDE.
However, the problem for :math:`\delta u` is still nonlinear and nothing is
gained. The idea is therefore to assume that :math:`\delta u` is sufficiently
small so that we can linearize the problem with respect to :math:`\delta u`.
Inserting :math:`u^{k+1}` in the PDE,
linearizing the :math:`q` term as

.. _Eq:_auto22:

.. math::

    \tag{37}
    q(u^{k+1}) = q(u^k) + q'(u^k)\delta u + {\cal O}((\delta u)^2)
        \approx q(u^k) + q'(u^k)\delta u,
        
        

and dropping nonlinear terms in :math:`\delta u`,
we get

.. math::
        
        \nabla\cdot\left( q(u^k)\nabla u^k\right) +
        \nabla\cdot\left( q(u^k)\nabla\delta u\right) +
        \nabla\cdot\left( q'(u^k)\delta u\nabla u^k\right) = 0{\thinspace .}
        

We may collect the terms with the unknown :math:`\delta u` on the left-hand side,

.. _Eq:_auto23:

.. math::

    \tag{38}
    \nabla\cdot\left( q(u^k)\nabla\delta u\right) +
        \nabla\cdot\left( q'(u^k)\delta u\nabla u^k\right) =
        -\nabla\cdot\left( q(u^k)\nabla u^k\right){\thinspace .}
        
        

We may state this equation using the code-friendly notation :math:`u^{-}` for :math:`u^k`:

.. _Eq:_auto24:

.. math::

    \tag{39}
    \nabla\cdot\left( q(u^{-})\nabla\delta u\right) +
        \nabla\cdot\left( q'(u^{-})\delta u\nabla u^{-}\right) =
        -\nabla\cdot\left( q(u^{-})\nabla u^{-}\right){\thinspace .}
        
        

The weak form of this PDE is derived by multiplying by a test function
:math:`v` and integrating over :math:`\Omega`, integrating as usual the
second-order derivatives by parts. This results in the variational
formulation: find :math:`u\in V` such that :math:`F(\delta u;v)=0\ \forall
v\in\hat V`, where

.. _Eq:ftut:nonlinear:Newton:pdelevel:F:

.. math::

    \tag{40}
    F= \int_\Omega \left(
        q(u^{-})\nabla\delta u\cdot \nabla v
        + q'(u^{-})\delta u\nabla u^{-}\cdot \nabla v\right) {\, \mathrm{d}x}
        -\int_\Omega q(u^{-})\nabla u^{-}\cdot \nabla v {\, \mathrm{d}x}{\thinspace .}
        
        

Note that this :math:`F(\delta u;v)` expression is linear in the unknown
:math:`\delta u`. We know that FEniCS can neatly extract the corresponding
bilinear form and the linear form by the ``lhs`` and ``rhs`` commands, so
in a program we prefer to state :math:`F` rather than the bilinear and
linear forms.

The function spaces :math:`V` and :math:`\hat V`, being continuous or discrete,
are as in the corresponding linear Poisson problem, see the section "Finite element
variational formulation": "" in [Ref1]_.

We must provide some initial guess. In the present problem, the
solution of the corresponding linear PDE is a natural choice.
The linear PDE corresponds to
:math:`q(u)=1`. The corresponding weak form :math:`a_0(u^0,v)=L_0(v)`
has

.. math::
        
        a_0(u,v)=\int_\Omega\nabla u\cdot \nabla v {\, \mathrm{d}x},\quad L_0(v)=0{\thinspace .}
        

Thereafter, we enter a loop and solve the linearized
:math:`F(\delta u;v)=0` for :math:`\delta u` and compute a new approximation
:math:`u^{-} + \delta u`. Note that :math:`\delta u` is a correction, so if
the initial guess :math:`u^0` satisfies the prescribed
Dirichlet conditions on some part :math:`\Gamma_D` of the boundary,
we must demand :math:`\delta u=0` on :math:`\Gamma_D`.

FEniCS implementation          (6)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Looking at :ref:`(40) <Eq:ftut:nonlinear:Newton:pdelevel:F>` we realize that
the variational form is the same as for the Newton method
at the algebraic level in the section :ref:`ftut:nonlinear:Newton:algebraic`. Since Newton's method at the
algebraic level required some "backward" construction of the
underlying weak forms, FEniCS users may prefer Newton's method at the
PDE level, which this author finds more straightforward, although not so
commonly documented in the literature on numerical methods for PDEs.
There is seemingly no need for differentiations to derive a Jacobian
matrix, but a mathematically equivalent derivation is done when
nonlinear terms are linearized using the first two Taylor series terms
and when products in the perturbation :math:`\delta u` are neglected.

.. index:: ft16_nlpoisson_pde_newton.py

The implementation is identical to the one in the section :ref:`ftut:nonlinear:Newton:algebraic` and is found in the file
`ft16_nlpoisson_pde_newton.py <https://github.com/hplgit/fenics-tutorial/blob/master/src/nlpoisson_pde_newton.py>`__. The reader is encouraged to
go through this code to be convinced that the present method actually
ends up with the same program
(`ft15_nlpoisson_alg_newton.py <https://github.com/hplgit/fenics-tutorial/blob/master/src/nlpoisson_alg_newton.py>`__) as needed for the Newton method at the
linear algebra level in the section :ref:`ftut:nonlinear:Newton:algebraic`.

Implementations with functions and classes
------------------------------------------

The implementations of the Picard and Newton methods so far in
this chapter have been done in terms of flat programs to maximize
the focus on the numerical details. However, for real-life
applications of the software you will most likely prefer to
have a solver function or problem and solver classes.

The Newton methods implemented as solver functions are found
in the file `ft13_nlpoisson_func.py <https://github.com/hplgit/fenics-tutorial/blob/master/src/nlpoisson_func.py>`__. The functions ``solver``,
``application_test`` and ``test_solver`` should be examined.
These represent a general solver, a special application, and a
series of unit tests, respectively.

A downside of the solver function mentioned is that it is quite restricted
with respect to boundary conditions and meshes. Much more flexibility
is enabled through a class implementation, although the code is also
much more lengthy. The file `ft13_nlpoisson_func.py <https://github.com/hplgit/fenics-tutorial/blob/master/src/nlpoisson_func.py>`__ contains all the details:

 * class ``NonlinearPoissonSolver`` for solving :math:`-\nabla\cdot (q(u)\nabla u)=f`
   on a general mesh with general Dirichlet and Neumann conditions,

 * class ``NonlinearPoissonProblem`` as an abstract super class for
   nonlinear Poisson problems,

 * class ``TestProblem`` for a specific implementation of the test problem
   used in this chapter.

There is also a comprehensive unit test for verifying all methods
through empirical convergence rate estimation. We strongly recommend
the reader to study the class implementation to identify its superiority
to other implementations in this chapter and to use this code base as
ideas for solving nonlinear problems with FEniCS.

.. --- begin exercise ---

.. _ftut:nonlinear:exer:Poissonwfu:

Exercise 3: Solve another nonlinear Poisson problem
---------------------------------------------------

We now consider the nonlinear PDE

.. math::
         -\nabla^2 u = f(u),

with Dirichlet, Neumann, or Robin conditions. Create some test problem
and an implementation to solve the problem.

.. --- end exercise ---

