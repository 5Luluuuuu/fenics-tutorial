========= The Poisson solver revisited =========
label{ch:poisson}

[hpl: I don't like this title, but have no other good alternative...]

======= Refactored implementation =======
label{ftut:poisson1:impl2}

Our first programs in this book are all ``flat''. That is,
they are not organized
into logical, reusable units in terms of Python functions. Such flat
programs are popular for quickly testing out some software, but not
well suited for serious problem solving. We shall therefore at once
*refactor* the program, meaning that we divide it into functions, but
this is just a reordering of the existing statements. During
refactoring, we try make functions as reusable as possible in other
contexts, but statements specific to a certain problem or task are
also encapsulated in (non-reusable) functions.  Being able to
distinguish reusable code from specialized code is a key issue when
refactoring code, and this ability depends on a good mathematical
understanding of the problem at hand (``what is general, what is
special?'').  In a flat program, general and specialized code (and
mathematics) is often mixed together.

===== A general solver function =====

We consider the flat program developed in Section ref{ftut:poisson1:impl}.
Some of the code in this program is needed to solve any
Poisson problem $-\nabla^2 u=f$ on $[0,1]\times [0,1]$ with $u=u_0$ on
the boundary, while other statements arise from our simple test
problem. Let us collect the general, reusable code in a function
`solver`.  Our special test problem will then just be an application
of `solver` with some additional statements.  We limit the `solver`
function to just *compute the numerical solution*. Plotting and
comparing the solution with the exact solution are considered to be
problem-specific activities to be performed elsewhere.

We parameterize `solver` by $f$, $u_0$, and the resolution of the
mesh. Since it is so trivial to use higher-order finite element
functions by changing the third argument to `FunctionSpace`, we let
also the degree of the polynomials in the finite element basis
functions be an argument to `solver`.

@@@CODE src/poisson_func.py fromto: from fenics import@def test_solver

=== Plotting for the test problem ===

The additional tasks we did in our initial program can be placed in
other functions. For example, plotting the solution in our particular
test problem is placed in an
`application_test` function:

@@@CODE src/poisson_func.py fromto: def application_test@if __name

=== Make a module! ===

The refactored code is put in a file "`${prog['poisson_func']}.py`":
"${src_url}/poisson/${prog['poisson_func']}.py". We should make
sure that such a file can be imported (and hence reused) in other programs.
Then all statements in the main program should appear with a test
`if __name__ == '__main__':`. This test is true if the file is executed
as a program, but false if the file is imported.
If we want to run this file in the same way as we can
run `${prog['poisson_func']}.py`, the main program is simply a call to
`application_test()` followed by a call `interactive()` to hold the plot:

@@@CODE src/poisson_func.py fromto: if __name@

idx{unit testing}

===== Verification and unit tests =====

The remaining part of our first program is to compare the numerical and
the exact solution. Every time we edit the code we must rerun the test
and examine that `max_error` is sufficiently small so we know that the
code still works. To this end, we shall adopt *unit testing*, meaning
that we create a mathematical test and corresponding software that
can run all our tests automatically and check that all tests pass.
Python has several tools for unit testing. Two very popular ones are
pytest and nose. These are almost identical and very easy to use.
More classical unit testing with test classes is offered by the built-in
tool `unittest`, but here we are going to use pytest (or nose) since it demands
shorter and clearer code.

Mathematically, our unit test is that the finite element solution of
our problem when $f=-6$ equals the exact solution $u=u_0=1+x^2+2y^2$.
We have already created code that finds the maximum error in the
numerical solution. Because of rounding errors, we cannot demand this
maximum error to be zero, but we have to use a tolerance, which depends
to the number of elements and the degrees of the polynomials in the finite
element basis functions. In Section ref{ftut:poisson1:impl:dissect} we
reported some experiments with the size of the maximum error. If we want
to test that `solver` works for meshes up to $2(20\times 20)$ elements
and cubic Lagrange elements, $10^{-11}$ is
an appropriate tolerance for testing that the maximum error vanishes.

Only three statements are necessary to carry out the unit test. However,
we shall embed these statements in software that the testing frameworks
pytest and nose can recognize. This means that each unit test
must be placed in a function that

 * has a name starting with `test_`
 * has no arguments
 * implements the test as `assert success, msg`

Regarding the last point, `success` is a boolean expression that is `False`
if the test fails, and in that case the string `msg` is written to the
screen. When the test fails, `assert` raises an `AssertionError` exception
in Python, otherwise the statement runs silently. The `msg` string is
optional, so `assert success` is the minimal test. In our case, we
will do `assert max_error < tol`, where `tol` is the tolerance ($10^{-11}$)
mentioned above.

A proper *test function* for implementing this unit test in the pytest
or nose testing frameworks has the following form. Note that we perform
the test for different mesh resolutions and degrees of finite elements.

@@@CODE src/poisson_func.py fromto: def test_solver@def application_test
We can at any time run

!bc sys
Terminal> py.test -s -v ${prog['poisson_func']}.py
!ec
and the pytest tool will run all functions `test_*()` in the file and report
how the tests go.

We shall make it a habit in this book to encapsulate numerical test
problems in unit tests as done above, and we strongly encourage the
reader to create similar unit tests whenever a FEniCS solver is
implemented. We dare to assert that this is the only serious way
do reliable computational science with FEniCS.

!bnotice Tip: Print messages in test functions
The `assert` statement runs silently when the test passes so users may
become uncertain if all the statements in a test function are really
executed. A psychological help is to print out something before `assert`
(as we do in the example above) such that it is clear that the
test really takes place.
(Note that `py.test` needs the `-s` option to show printout
from the test functions.)
!enotice

The next three sections deal with some technicalities about specifying
the solution method for linear systems (so that you can solve large
problems) and examining array data from the computed solution (so that
you can check that the program is correct).  These technicalities are
scattered around in forthcoming programs. However, the impatient
reader who is more interested in seeing the previous program being
adapted to a real physical problem, and play around with some
interesting visualizations, can safely jump to Section
ref{ftut:poisson:membrane}.  Information in the intermediate sections
can be studied on demand.

% if EXV:
===== Exercise: Solve a Poisson problem =====

Solve the following problem

!bt
\begin{align}
\nabla^2 u &= 2e^{-2x}\sin(\pi y)((4-5\pi^2)\sin(2\pi x) - 8\pi\cos(2\pi x))
\hbox{ in }\Omega = [0,1]\times [0,1]\\
u &= 0\quad\hbox{ on }\partial\Omega
\end{align}
!et
The exact solution is given by

!bt
\[ u(x,y) = 2e^{-2x}\sin(\pi x)\sin(\pi y)\tp\]
!et
Compute the maximum numerical approximation error in a mesh with
$2(N_x\times N_y)$ elements and in a mesh with double resolution:
$4(N_x\times N_y)$ elements. Show that the doubling the resolution
reduces the error by a factor 4 when using Lagrange elements of degree one.
Make an illustrative plot of the solution too.

!bsubex
file=poisson_fsin_flat

Base your implementation on editing the program
`${prog["poisson_flat"]}.py`.

!bhint
In the string for an `Expression` object, `pi` is the value of
$\pi$. Also note that $\pi^2$ must be expressed with syntax
`pow(pi,2)` and not (the common Python syntax) `pi**2`.

FEniCS will abort with a compilation error if you type the expressions
in a wrong way syntax-wise.  Search for *error:* in the
`/very/long/path/compile.log` file mentioned in the error message to
see what the C++ compiler reported as error in the expressions.
!ehint

!bhint
The result that with P1 elements, doubling the resolution reduces the error
with a factor of four, is an
asymptotic result so it requires a sufficiently fine mesh. Here
one may start with $N_x=N_y=20$.
!ehint

!bsol
Looking at the `${prog["poisson_flat"]}.py` code, we realize that
the following edits are required:

 * Modify the `mesh` computation.
 * Modify `u0` and `f`.
 * Add expression for the exact solution.
 * Modify the computation of the numerical error.
 * Insert a loop to enable solving the problem twice.
 * Put the error reduction computation and the plot statements after the loop.

Here is the modified code:

@@@CODE exer/poisson_fsin_flat.py fromto: from fenics import@

The number $\pi$ has the symbol `M_PI` in C and C++, but in C++
strings in `Expression` objects, the symbol `pi` can be used directly
(or one can use the less readable `DOLFIN_PI`).

FIGURE: [fig/poisson_fsin, width=500 frac=0.8]

!esol
!esubex

!bsubex
file=poisson_fsin_func

Base your implementation on a new file that imports functionality
from the module `${prog["poisson_func"]}.py`. Embed the check of the
reduction of the numerical approximation error in a unit test.

!bsol
Solving the two problems is a matter of calling `solver` with
different sets of arguments.
To compute the numerical error,
we need code that is close to what we have in `test_solver`.

@@@CODE exer/poisson_fsin_func.py fromto: from poisson_func import@
The unit test is embedded in a proper test function `test_solver`
for the pytest or
nose testing frameworks. Visualization of the solution is encapsulated
in the `application` function. Since we need `u_e`, `u0`, and `f`
in two functions, we place the definitions in a function `data` to
avoid copies of these expressions.

!esol
!esubex

!bremarks
This exercise demonstrates that changing a flat program to solve a new
problem requires careful editing of statements scattered around in the
file, while
the solution in b), based on the `solver` function, requires *no modifications*
of the `${prog["poisson_func"]}.py` file, just
*minimalistic additional new code* in a separate file. The Poisson solver
remains in one place (`${prog["poisson_func"]}.py`) while in a) we got two
Poisson solvers. If you decide to switch to an iterative solution method
for linear systems, you can do so in one place in b), and all applications
can take advantage of the extension. Hopefully, with this exercise
you realize that embedding
PDE solvers in functions (or classes) makes more reusable software than
flat programs.
!eremarks
% endif

% if EXV:
===== Exercise: Refactor the code for membrane deflection =====
label{ftut:poisson:exer:membrane}

The `${prog["poisson_flat_membrane"]}.py` simulates the deflection of
a membrane. Refactor this code such that we have a `solver` function
as in the `${prog["poisson_func"]}.py` file. Let the user have the
option to choose a direct or iterative solver for the linear system.
Also implement a unit test where you have $p=4$ (constant) and
use P2 and P3 elements. In this case, the exact solution is
quadratic in $x$ and $y$ and will be ``exactly'' reproduced by
P2 and higher-order elements.

!bsol
We can use the `solver` function from `${prog["poisson_func"]}.py`
right away. The major difference is that
the domain is now a circle and not a square. We change the `solver`
function by letting the mesh be an argument `mesh` (instead of `Nx`
and `Ny`):

!bc pycod
def solver(
    f, u0, mesh, degree=1,
    linear_solver='Krylov', # Alt: 'direct'
    ...):
    V = FunctionSpace(mesh, 'P', degree)
    # code as before
!ec

The complete code becomes

@@@CODE exer/poisson_membrane.py fromto: def application@def test_membrane
The key function to simulate membrane deflection is named `application`.

For $p=4$, we have $w=1-x^2-y^2$ as exact solution.
The unit test for P2 and P3 goes as follows:

@@@CODE exer/poisson_membrane.py fromto: def test_membrane@if __name
The striking feature is that the solver does not reproduce the solution
to an accuracy more than about 0.01 (!), regardless of the resolution and
type of element.

## Seems as this solution is exactly reproduced very quickly, but
## with some significant inaccuracy of the size 0.01.

!esol
% endif


!split
# Or Useful extensions and recipies?
======= Useful extensions =======

[hpl: Need a little intro.]


===== Controlling the solution process =====
label{ftut:poisson1:solve:prm}

Sparse LU decomposition (Gaussian elimination) is used by default to
solve linear systems of equations in FEniCS programs.  This is a very
robust and recommended method for a few thousand unknowns in the
equation system, and may hence be the method of choice in many 2D and
smaller 3D problems. However, sparse LU decomposition becomes slow and
memory demanding in large problems.  This fact forces the use of
iterative methods, which are faster and require much less memory.
The forthcoming text tells you how to
advantage of state-of-the-art iterative solution methods in FEniCS.

=== Setting linear solver parameters ===

Preconditioned Krylov solvers is a type of popular iterative methods
that are easily accessible in FEniCS programs. The Poisson equation
results in a symmetric, positive definite coefficient matrix, for
which the optimal Krylov solver is the Conjugate Gradient (CG)
method. However, the CG method requires boundary conditions to be
implemented in a symmetric way. This is not the case by default, so
then a Krylov solver for non-symmetric system, such as GMRES, is a
better choice.  Incomplete LU factorization (ILU) is a popular and
robust all-round preconditioner, so let us try the GMRES-ILU pair:

!bc pycod
solve(a == L, u, bc)
      solver_parameters={'linear_solver': 'gmres',
                         'preconditioner': 'ilu'})
# Alternative syntax
solve(a == L, u, bc,
      solver_parameters=dict(linear_solver='gmres',
                             preconditioner='ilu'))
!ec
Section ref{ftut:app:solver:prec} lists the most popular choices of
Krylov solvers and preconditioners available in FEniCS.

idx{linear algebra backend}
idx{PETSc} idx{Trilinos} idx{MTL4} idx{uBLAS}

=== Linear algebra backend ===

The actual GMRES and ILU implementations that are brought into action
depends on the choice of linear algebra package. FEniCS interfaces
several linear algebra packages, called *linear algebra backends* in
FEniCS terminology.  PETSc is the default choice if FEniCS is compiled
with PETSc, otherwise uBLAS.  Epetra (Trilinos), Eigen, MTL4 are other
supported backends. Which backend to apply can be controlled by
setting

!bc pycod
parameters['linear_algebra_backend'] = backendname
!ec
where `backendname` is a string, either `'Eigen'`, `'PETSc'`, `'uBLAS'`,
`'Epetra'`, or `'MTL4'`.  All these backends offer high-quality
implementations of both iterative and direct solvers for linear systems
of equations.

idx{UMFPACK}

A common platform for FEniCS users is Ubuntu Linux.  The FEniCS
distribution for Ubuntu contains PETSc, making this package the
default linear algebra backend.  The default solver is sparse LU
decomposition (`'lu'`), and the actual software that is called is then
the sparse LU solver from UMFPACK (which PETSc has an interface
to). The available linear algebra backends in a FEniCS installation is
listed by

!bc pycod
list_linear_algebra_backends()
!ec

idx{`parameters` database}
idx{`info` function}

=== The `parameters` database ===

We will normally like to control the tolerance in the stopping
criterion and the maximum number of iterations when running an
iterative method.  Such parameters can be set by accessing the *global
parameter database*, which is called `parameters` and which behaves as
a nested dictionary. Write

!bc pycod
info(parameters, verbose=True)
!ec
to list all parameters and their default values in the database.
The nesting of parameter sets is indicated through indentation in the
output from `info`.
According to this output, the relevant parameter set is
named `'krylov_solver'`, and the parameters are set like this:

!bc pycod
prm = parameters['krylov_solver']  # short form
prm['absolute_tolerance'] = 1E-10
prm['relative_tolerance'] = 1E-6
prm['maximum_iterations'] = 1000
!ec
Stopping criteria for Krylov solvers usually involve the norm of
the residual, which must be smaller than the absolute tolerance
parameter *or* smaller than the relative tolerance parameter times
the initial residual.

To get a printout of the number of actual iterations to reach the
stopping criterion, we can insert

!bc pycod
set_log_level(PROGRESS)
# or
set_log_level(DEBUG)
!ec
A message with the equation system size, solver type, and number of
iterations arises from specifying the argument `PROGRESS`, while
`DEBUG` results in more information, including CPU time spent in
the various parts of the matrix assembly and solve process.

We remark that default values for the global parameter database can be
defined in an XML file. To generate such a file from the current set
of parameters in a program, run

!bc pycod
File('fenics_parameters.xml') << parameters
!ec
If a `fenics_parameters.xml` file is found in the directory where a
FEniCS program is run, this file is read and used to initialize the
`parameters` object. Otherwise, the file
`.config/fenics/fenics_parameters.xml` in the user's home directory is
read, if it exists.  Another alternative is to load the XML (with any
name) manually in the program:

!bc pycod
File('fenics_parameters.xml') >> parameters
!ec
The XML file can also be in gzip'ed form with the extension `.xml.gz`.


idx{`${prog["poisson_iter"]}.py`}

=== An extended solver function ===

Let us extend the previous solver function from
`${prog["poisson_func"]}.py` such that it also offers the GMRES+ILU
preconditioned Krylov solver.

@@@CODE src/poisson_iter.py fromto: from fenics import@def test_solver
This new `solver` function, found in the file
`${prog["poisson_iter"]}.py`, replaces the one in `${prog["poisson_func"]}.py`:
it has all the functionality of the previous `solver` function,
but can also solve the linear system with
iterative methods and report the progress of such solvers.

=== Remark regarding unit tests ===

Regarding verification of the new `solver` function in terms of unit
tests, it turns out that unit testing in a problem where the
approximation error vanishes is gets more complicated when we use
iterative methods. The problem is to keep the error due to iterative
solution smaller than the tolerance used in the verification
tests. First of all this means that the tolerances used in the Krylov
solvers must be smaller than the tolerance used in the `assert` test,
but this is no guarantee to keep the linear solver error this small.
For linear elements and small meshes, a tolerance of $10^{-11}$ works
well in the case of Krylov solvers too (using a tolerance $10^{-12}$
in those solvers. However, as soon as we switch to P2 elements, it is
hard to force the linear solver error below $10^{-6}$. Consequently,
tolerances in tests depend on the numerical methods. The interested
reader is referred to the `test_solver` function in
`${prog["poisson_iter"]}.py` for details: this test function tests the
numerical solution for direct and iterative linear solvers, for
different meshes, and different degrees of the polynomials in the
finite element basis functions.


===== Linear variational problem and solver objects =====
label{ftut:poisson1:solver:problem}
idx{LinearVariationalProblem}
idx{LinearVariationalSolver}

idx{`${prog["poisson_iter"]}.py`}

The `solve(a == L, u, bc)` call is just a compact syntax alternative to a
slightly more comprehensive specification of the variational equation
and the solution of the associated linear system.  This alternative
syntax is used in a lot of FEniCS applications and will also be
used later in this tutorial, so we show it already now:

!bc pycod
u = Function(V)
problem = LinearVariationalProblem(a, L, u, bc)
solver  = LinearVariationalSolver(problem)
solver.solve()
!ec

Many objects have an attribute `parameters` corresponding to
a parameter set in the global `parameters` database,
but local to the object. Here, `solver.parameters` play that
role. Setting the CG method with ILU preconditioning as solution
method and specifying solver-specific parameters can be done
like this:

!bc pycod
solver.parameters['linear_solver'] = 'gmres'
solver.parameters['preconditioner'] = 'ilu'
prm = solver.parameters['krylov_solver'] # short form
prm['absolute_tolerance'] = 1E-7
prm['relative_tolerance'] = 1E-4
prm['maximum_iterations'] = 1000
!ec
Settings in the global `parameters` database are
propagated to parameter sets in individual objects, with the
possibility of being overwritten as done above.

The linear variational problem and solver objects as outlined above
are incorporated in an alternative solver function, named
`solver_objects`, in
`${prog["poisson_iter"]}.py`. Otherwise, this function is parallel to the
previously shown `solver` function.


===== Writing out the discrete solution =====
label{ftut:poisson1:verify1}

We have seen how to grab the degrees of freedom array from a
finite element function `u`:

!bc pycod
u_array = u.vector().array()
!ec
The elements in `u_array` correspond to function values of `u` at nodes
in the mesh.  Now, a fundamental question is: What are the
coordinates of node `i` whose value is `u_array[i]`? To answer this
question, we need to understand how to get our hands on the
coordinates, and in particular, the numbering of degrees of freedom
and the numbering of vertices in the mesh. We start with P1 (1st order
Lagrange) elements where all the nodes are vertices in the mesh.

The function `mesh.coordinates()` returns the coordinates of the
vertices as a `numpy` array with shape $(M,d$), $M$ being the number
of vertices in the mesh and $d$ being the number of space dimensions:

!bc pyshell
>>> from fenics import *
>>>
>>> mesh = UnitSquareMesh(2, 2)
>>> coor = mesh.coordinates()
>>> coor
array([[ 0. ,  0. ],
       [ 0.5,  0. ],
       [ 1. ,  0. ],
       [ 0. ,  0.5],
       [ 0.5,  0.5],
       [ 1. ,  0.5],
       [ 0. ,  1. ],
       [ 0.5,  1. ],
       [ 1. ,  1. ]])
!ec
We see from this output that vertices are first numbered along $y=0$
with increasing $x$ coordinate, then along $y=0.5$, and so on.

Next we compute a function `u` on this mesh, e.g., the $u=x+y$:

!bc pyshell
>>> V = FunctionSpace(mesh, 'P', 1)
>>> u = interpolate(Expression('x[0]+x[1]'), V)
>>> plot(u, interactive=True)
>>> u_array = u.vector().array()
>>> u_array
array([ 1. ,  0.5,  1.5,  0. ,  1. ,  2. ,  0.5,  1.5,  1. ])
!ec
We observe that `u_array[0]` is *not* the value of $x+y$ at vertex number 0,
since this vertex has coordinates $x=y=0$. The numbering of the
degrees of freedom $U_1,\ldots,U_{N}$ is obviously not the same as the
numbering of the vertices.

In the plot of `u`, type `w` to turn on wireframe instead of fully colored
surface, `m` to show the mesh, and then `v` to show the
numbering of the vertices.

<linebreak>
<linebreak>

FIGURE: [fig/vertex_numbering, width=500 frac=0.8]

<linebreak>
<linebreak>

idx{compute vertex values}
idx{vertex values}

The vertex values of a `Function` object can be extracted by
`u.compute_vertex_values()`, which returns an array where element `i`
is the value of `u` at vertex `i`:

!bc pyshell
>>> u_at_vertices = u.compute_vertex_values()
>>> for i, x in enumerate(coor):
...     print('vertex %d: u_at_vertices[%d]=%g\tu(%s)=%g' %
...           (i, i, u_at_vertices[i], x, u(x)))
vertex 0: u_at_vertices[0]=0	u([ 0.  0.])=8.46545e-16
vertex 1: u_at_vertices[1]=0.5	u([ 0.5  0. ])=0.5
vertex 2: u_at_vertices[2]=1	u([ 1.  0.])=1
vertex 3: u_at_vertices[3]=0.5	u([ 0.   0.5])=0.5
vertex 4: u_at_vertices[4]=1	u([ 0.5  0.5])=1
vertex 5: u_at_vertices[5]=1.5	u([ 1.   0.5])=1.5
vertex 6: u_at_vertices[6]=1	u([ 0.  1.])=1
vertex 7: u_at_vertices[7]=1.5	u([ 0.5  1. ])=1.5
vertex 8: u_at_vertices[8]=2	u([ 1.  1.])=2
!ec

idx{vertex to dof map}
idx{dof to vertex map}

Alternatively, we can ask for the mapping from vertex numbering to degrees
of freedom numbering in the space $V$:

!bc
v2d = vertex_to_dof_map(V)
!ec
Now, `u_array[v2d[i]]` will give us the value of the
degree of freedom in `u` corresponding
to vertex `i` (`v2d[i]`). In particular, `u_array[v2d]` is an array
with all the elements in the same (vertex numbered) order as `coor`.
The inverse map, from degrees of freedom
number to vertex number is given by `dof_to_vertex_map(V)`, so
`coor[dof_to_vertex_map(V)]` results in an array of all the
coordinates in the same order as the degrees of freedom.

For Lagrange elements of degree larger than 1, there are degrees of
freedom (nodes) that do not correspond to vertices.
[hpl: Anders, is the following true?] There is no simple way of getting the
coordinates associated with the non-vertex degrees of freedom, so
if we want to write out the values of a finite element solution,
the following code snippet does the task at the vertices, and this
will work for all kinds of Lagrange elements.

@@@CODE src/poisson_iter.py fromto: def compare_exact@def normalize
As expected, the error is either identically zero or about $10^{-15}$ or
$10^{-16}$.

!bwarning Cheap vs expensive function evaluation
Given a `Function` object `u`, we can evaluate its values in various
ways:

 o `u(x)` for an arbitrary point `x`
 o `u.vector().array()[i]` for degree of freedom number `i`
 o `u.compute_vertex_values()[i]` at vertex number `i`

The first method, though very flexible, is in general very expensive
while the other two are very efficient (but limited to certain points).
!ewarning

To demonstrate the use of point evaluations of `Function` objects,
we write out the computed `u` at the center point
of the domain and compare it with the exact solution:

!bc pycod
center = (0.5, 0.5)
error = u0(center) - u(center)
print('numerical error at %s: %g' % (center, error)
!ec
Trying a $2(3\times 3)$ mesh, the output from the
previous snippet becomes

!bc dat
numerical error at (0.5, 0.5): -0.0833333
!ec
The discrepancy is due to the fact that the center point is not a node
in this particular mesh, but a point in the interior of a cell,
and `u` varies linearly over the cell while
`u0` is a quadratic function. When the center point is a node, as in
a $2(t\times 2)$ or $2(4\times 4)$ mesh, the error is of the order
$10^{-15}$.

We have seen how to extract the nodal values in a `numpy` array.
If desired, we can adjust the nodal values too. Say we want to
normalize the solution such that $\max_j U_j = 1$. Then we
must divide all $U_j$ values
by $\max_j U_j$. The following function performs the task:

@@@CODE src/poisson_iter.py fromto: def normalize@def test_normalize
That is, we manipulate `u_array` as desired, and then we insert this
array into `u`'s `Vector` object.  The `/=` operator implies an
in-place modification of the object on the left-hand side: all
elements of the `u_array` are divided by the value `max_u`.
Alternatively, one could write `u_array = u_array/max_u`, which
implies creating a new array on the right-hand side and assigning this
array to the name `u_array`.

!bwarning Be careful when manipulating degrees of freedom
A call like `u.vector().array()` returns a *copy* of the data in
`u.vector()`. One must therefore never perform assignments like
`u.vector.array()[:] = ...`, but instead extract the `numpy` array
(i.e., a copy), manipulate it, and insert it back with `u.vector()[:]
= ` or `u.set_local(...)`.
!ewarning


All the code in this subsection can be found in the file `${prog["poisson_iter"]}.py`.


===== Parameterizing the number of space dimensions =====
label{ftut:poisson:nD}
idx{dimension-independent code}

FEniCS makes it is easy to write a unified simulation code that can
operate in 1D, 2D, and 3D. We will conveniently make use of this
feature in forthcoming examples.  As an appetizer, go back to the
introductory programs `${prog["poisson_flat"]}.py` or
`${prog["poisson_func"]}.py` and change the
mesh construction from `UnitSquareMesh(6, 4)` to `UnitCubeMesh(6, 4,
5)`. Now the domain is the unit cube partitioned into $6\times 4\times
5$|$6x4x5$ boxes, and each box is divided into six tetrahedra-shaped
finite elements for computations.  Run the program and observe that we
can solve a 3D problem without any other modifications (!). The
visualization allows you to rotate the cube and observe the function
values as colors on the boundary.


=== Generating a hypercube ===

The syntax for generating a unit interval, square, or box is different,
so we need to encapsulate this part of the code. Given a list or
tuple with the divisions into cells in the various spatial direction,
the following function returns the mesh in a $d$-dimensional problem:

!bc pycod
def unit_hypercube(divisions, degree):
    mesh_classes = [UnitIntervalMesh, UnitSquareMesh, UnitCubeMesh]
    d = len(divisions)
    mesh = mesh_classes[d-1](*divisions)
    V = FunctionSpace(mesh, 'P', degree)
    return V, mesh
!ec
The construction `mesh_class[d-1]` will pick the right name of the
object used to define the domain and generate the mesh.
Moreover, the argument `*divisions`
sends all the component of the list `divisions` as separate
arguments. For example, in a 2D problem where `divisions` has
two elements, the statement

!bc pycod
mesh = mesh_classes[d-1](*divisions)
!ec
is equivalent to

!bc pycod
mesh = UnitSquareMesh(divisions[0], divisions[1])
!ec

Replacing the `Nx` and `Ny` parameters by `divisions` and calling
`unit_hypercube` to create the mesh are the two modifications that
we need in any of the previously shown `solver` functions to turn
them into solvers for $d$-dimensional problems!


===== Computing derivatives =====
label{ftut:poisson:gradu}

idx{projection}

In Poisson and many other problems, the gradient of the solution is
of interest. The computation is in principle simple:
since
$u = \sum_{j=1}^N U_j \phi_j$, we have that

!bt
\begin{equation*}
\nabla u = \sum_{j=1}^N U_j \nabla \phi_j\tp
\end{equation*}
!et
Given the solution variable `u` in the program, its gradient is
obtained by `grad(u)` or `grad(u)`.  However, the gradient of a
piecewise continuous finite element scalar field is a discontinuous
vector field since the $\phi_j$ has discontinuous derivatives at the
boundaries of the cells. For example, using Lagrange elements of
degree 1, $u$ is linear over each cell, and the numerical $\nabla u$
becomes a piecewise constant vector field. On the contrary, the exact
gradient is continuous.  For visualization and data analysis purposes
we often want the computed gradient to be a continuous vector
field. Typically, we want each component of $\nabla u$ to be
represented in the same way as $u$ itself. To this end, we can project
the components of $\nabla u$ onto the same function space as we used
for $u$.  This means that we solve $w = \nabla u$ approximately by a
finite element method, using the same elements for the components of
$w$ as we used for $u$. This process is known as *projection*.

idx{`project`} idx{projection}

Not surprisingly, projection is a so common operation in finite
element programs that FEniCS has a function for doing the task:
`project(q, W)`, which returns the projection of some `Function` or
`Expression` object named `q` onto the `FunctionSpace` (if `q` is
scalar) or `VectorFunctionSpace` (if `q` is vector-valued) named `W`.
Specifically, in our case where `u` is computed and we want to project
the vector-valued `grad(u)` onto the `VectorFunctionSpace` where each
component has the same `Function` space as `u`:

!bc pycod
V = u.function_space()
degree = u.ufl_element().degree()
W = VectorFunctionSpace(V.mesh(), 'P', degree)

grad_u = project(grad(u), W)
!ec
Figure ref{ftut:poisson:2D:fig:ex1:gradu} shows
example of how such a smoothed `gradu(u)` vector field is visualized.

FIGURE:[fig/ex1_gradu, width=480] Example of visualizing the vector field $\nabla u$ by arrows at the nodes. label{ftut:poisson:2D:fig:ex1:gradu}

The applications of projection are many, including turning discontinuous
gradient fields into continuous ones, comparing higher- and lower-order
function approximations, and transforming a higher-order finite element
solution down to a piecewise linear field, which is required by many
visualization packages.


The scalar component fields of the gradient
can be extracted as separate fields and, e.g., visualized:

!bc pycod
grad_u_x, grad_u_y = grad_u.split(deepcopy=True)
plot(grad_u_x, title='x-component of grad(u)')
plot(grad_u_y, title='y-component of grad(u)')
!ec
The `deepcopy=True` argument signifies a *deep copy*, which is
a general term in computer science implying that a copy of the data is
returned. (The opposite, `deepcopy=False`,
means a *shallow copy*, where
the returned objects are just pointers to the original data.)

idx{degrees of freedom array}
idx{nodal values array}
idx{degrees of freedom array (vector field)}

The `grad_u_x` and `grad_u_y` variables behave as
`Function` objects. In particular, we can extract the underlying
arrays of nodal values by

!bc pycod
grad_u_x_array = grad_u_x.vector().array()
grad_u_y_array = grad_u_y.vector().array()
!ec
The degrees of freedom of the `grad_u` vector field can also be
reached by

!bc pycod
grad_u_array = grad_u.vector().array()
!ec
but this is a flat `numpy` array where the degrees of freedom for the
$x$ component of the gradient is stored in the first part, then the
degrees of freedom of the $y$ component, and so on. This is less convenient
to work with.

idx{`${prog["poisson_iter"]}.py`}

The function `gradient(u)` in `${prog["poisson_iter"]}.py`
returns a projected (smoothed) $\nabla u$ vector field, given some
finite element function `u`:

@@@CODE src/poisson_iter.py fromto: def gradient@def application_test_gradient

Examining the arrays with vertex values of `grad_u_x` and `grad_u_y`
quickly reveals that the computed `grad_u` field does not equal the
exact gradient $(2x, 4y)$ in this particular test problem where
$u=1+x^2+2y^2$.  There are inaccuracies at the boundaries, arising
from the approximation problem for $w$. Increasing the mesh resolution
shows, however, that the components of the gradient vary linearly as
$2x$ and $4y$ in the interior of the mesh (i.e., as soon as we are one
element away from the boundary).  The `application_test_gradient`
function in `${prog["poisson_iter"]}.py` performs some experiments.

!bnotice Detour: Manual projection.
Although you will always use `project` to project a finite element
function, it can be constructive this point in the tutorial to formulate the
projection mathematically and implement its steps manually in FEniCS.

Looking at the component $\partial u/\partial x$ of the gradient, we
project the (discrete) derivative $\sum_jU_j{\partial \phi_j/\partial
x}$ onto a function space with basis $\phi_1,\phi_2,\ldots$ such that
the derivative in this space is expressed by the standard sum
$\sum_j\bar U_j \phi_j$, for suitable (new) coefficients $\bar U_j$.

The variational problem for $w$ reads: find  $w\in \Vg$ such that

!bt
\begin{equation}
a(w, v) = L(v)\quad\forall v\in \hat{\Vg},
\end{equation}
!et
where

!bt
\begin{align}
a(w, v) &= \int_\Omega w\cdot v \dx,\\
L(v) &= \int_\Omega \nabla u\cdot v \dx\tp
\end{align}
!et
The function spaces $\Vg$ and $\hat{\Vg}$ (with the superscript g
denoting ``gradient'') are vector versions of the function space for
$u$, with boundary conditions removed (if $V$ is the space we used for
$u$, with no restrictions on boundary values, $\Vg = \hat{\Vg} =
[V]^d$, where $d$ is the number of space dimensions).  For example, if
we used piecewise linear functions on the mesh to approximate $u$, the
variational problem for $w$ corresponds to approximating each
component field of $w$ by piecewise linear functions.

The variational problem for the vector field
$w$, called `grad_u` in the code, is easy to solve in FEniCS:

!bc pycod
V_g = VectorFunctionSpace(mesh, 'P', 1)
w = TrialFunction(V_g)
v = TestFunction(V_g)

a = dot(w, v)*dx
L = dot(grad(u), v)*dx
grad_u = Function(V_g)
solve(a == L, grad_u)

plot(grad_u, title='grad(u)')
!ec
The boundary condition argument to `solve` is dropped since there are
no essential boundary conditions in this problem.
The new thing is basically that we work with a `VectorFunctionSpace`,
since the unknown is now a vector field, instead of the
`FunctionSpace` object for scalar fields.
!enotice


===== A variable-coefficient Poisson problem =====
label{ftut:possion:2D:varcoeff}
idx{Poisson's equation with variable coefficient}
idx{`${prog["poisson_vc"]}.py`}

Suppose we have a variable coefficient $p(x,y)$ in the Laplace operator,
as in the boundary-value problem

!bt
\begin{equation} label{ftut:poisson:2D:varcoeff}
  \begin{split}
    - \nabla\cdot \left\lbrack
p(x,y)\nabla u(x,y)\right\rbrack &= f(x,y) \quad \mbox{in } \Omega,
    \\
    u(x,y) &= u_0(x,y) \quad \mbox{on}\  \partial\Omega\tp
  \end{split}
\end{equation}
!et
We shall quickly demonstrate that this simple extension of our model
problem only requires an equally simple extension of the FEniCS program.

=== Test problem ===

Let us continue to use our favorite solution $u(x,y)=1+x^2+2y^2$ and
then prescribe $p(x,y)=x+y$. It follows that
$u_0(x,y) = 1 + x^2 + 2y^2$ and $f(x,y)=-8x-10y$.

=== Modifications of the PDE solver ===

What are the modifications we need to do in the previously shown codes
to incorporate the variable coefficient $p$?
from Section ref{ftut:poisson1:verify1}?

  * `solver` must take `p` as argument,
  * `f` in our test problem
    must be an `Expression` since it is no longer a constant,
  * a new `Expression p` must be defined for the variable coefficient,
  * the formula for $a(u,v)$ in the variational problem is slightly changed.

First we address the modified variational problem. Multiplying
the PDE by a test function $v$ and
integrating by parts now results
in

!bt
\begin{equation*}
\int_\Omega p\nabla u\cdot\nabla v \dx -
\int_{\partial\Omega} p{\partial u\over
\partial n}v \ds = \int_\Omega fv \dx\tp
\end{equation*}
!et
The function spaces for $u$ and $v$ are the same as in
Section ref{ftut:poisson1:varform}, implying that the boundary integral
vanishes since $v=0$ on $\partial\Omega$ where we have Dirichlet conditions.
The weak form $a(u,v)=L(v)$ then has

!bt
\begin{align}
a(u,v) &= \int_\Omega p\nabla u\cdot\nabla v \dx,\\
L(v) &= \int_\Omega fv \dx\tp
\end{align}
!et
In the code for solving $-\nabla^2u=f$ we must replace

!bc pycod
a = dot(grad(u), grad(v))*dx
!ec
by

!bc pycod
a = p*dot(grad(u), grad(v))*dx
!ec
to solve $-\nabla\cdot(p\nabla u)=f$. Moreover,
the definitions of `p` and `f` in the test problem read

!bc pycod
p = Expression('x[0] + x[1]')
f = Expression('-8*x[0] - 10*x[1]')
!ec
No additional modifications are necessary. The file
`${prog["poisson_vc"]}.py` (variable-coefficient Poisson problem in 2D)
is a copy of `${prog["poisson_iter"]}.py` with the mentioned changes
incorporated. Observe that $p=1$ recovers the original problem in
`${prog["poisson_iter"]}.py`.

You can run it and confirm
that it recovers the exact $u$ at the nodes.

=== Modifications of the flux computations ===

The flux $-p\nabla u$ may be of particular interest in
variable-coefficient Poisson problems as it often has an interesting
physical significance. As explained in Section ref{ftut:poisson:gradu},
we normally want the piecewise discontinuous flux or gradient to be
approximated by a continuous vector field, using the same elements as
used for the numerical solution $u$. The approximation now consists of
solving $w = -p\nabla u$ by a finite element method: find $w\in \Vg$
such that

!bt
\begin{equation}
a(w, v) = L(v)\quad\forall v\in \hat{\Vg},
\end{equation}
!et
where

!bt
\begin{align}
a(w, v) &= \int_\Omega w\cdot v \dx,\\
L(v) &= \int_\Omega (-p \nabla u)\cdot v \dx\tp
\end{align}
!et
This problem is identical to the one in Section ref{ftut:poisson:gradu},
except that $p$ enters the integral in $L$.

The relevant Python statement for computing the flux field take the form

!bc pycod
flux = project(-p*grad(u),
               VectorFunctionSpace(mesh, 'P', degreee))
!ec
An appropriate function for computing the flux based on `u` and `p` is

@@@CODE src/poisson_vc.py fromto: def flux@def application_test_gradient


Plotting the flux vector field is naturally as easy as plotting
the gradient (see Section ref{ftut:poisson:gradu}):

!bc pycod
plot(flux, title='flux field')

flux_x, flux_y = flux.split(deepcopy=True)  # extract components
plot(flux_x, title='x-component of flux (-p*grad(u))')
plot(flux_y, title='y-component of flux (-p*grad(u))')
!ec

For data analysis of the nodal values of the flux field we can
grab the underlying `numpy` arrays (demands a `deepcopy=True`
in the split of `flux`):

!bc pycod
flux_x_array = flux_x.vector().array()
flux_y_array = flux_y.vector().array()
!ec

The function `application_test_flux` in the
program `${prog["poisson_vc"]}.py` demonstrates the computations described
above.

===== Creating the linear system explicitly =====
label{ftut:poisson1:linalg}

Given $a(u,v)=L(v)$, the discrete solution $u$ is computed by
inserting $u=\sum_{j=1}^N U_j \phi_j$ into $a(u,v)$ and demanding
$a(u,v)=L(v)$ to be fulfilled for $N$ test functions
$\hat\phi_1,\ldots,\hat\phi_N$. This implies

!bt
\begin{equation*}
\sum_{j=1}^N a(\phi_j,\hat\phi_i) U_j = L(\hat\phi_i),\quad i=1,\ldots,N,
\end{equation*}
!et
which is nothing but a linear system,

!bt
\begin{equation*}
  AU = b,
\end{equation*}
!et
where the entries in $A$ and $b$ are given by

!bt
\begin{align*}
  A_{ij} &= a(\phi_j, \hat{\phi}_i), \\
  b_i &= L(\hat\phi_i)\tp
\end{align*}
!et

idx{`assemble`}
idx{linear systems (in FEniCS)}
idx{assembly of linear systems}

The examples so far have specified the left- and right-hand side of
the variational formulation and then asked FEniCS to assemble the
linear system and solve it.  An alternative is to explicitly call
functions for assembling the coefficient matrix $A$ and the right-side
vector $b$, and then solve the linear system $AU=b$ with respect to
the $U$ vector.  Instead of `solve(a == L, u, b)` we now write

!bc pycod
A = assemble(a)
b = assemble(L)
bc.apply(A, b)
u = Function(V)
U = u.vector()
solve(A, U, b)
!ec
The variables `a` and `L` are as before. That is, `a` refers to the
bilinear form involving a `TrialFunction` object (e.g., `u`)
and a `TestFunction` object (`v`), and `L` involves a
`TestFunction` object (`v`). From `a` and `L`,
the `assemble` function can
compute $A$ and $b$.

The matrix $A$ and vector $b$ are first assembled without incorporating
essential (Dirichlet) boundary conditions. Thereafter, the
call `bc.apply(A, b)` performs the necessary modifications of
the linear system such that `u` is guaranteed to equal the prescribed
boundary values.
When we have multiple Dirichlet conditions stored in a list `bcs`,
as explained in Section ref{ftut:poisson:multiple:Dirichlet}, we must apply
each condition in `bcs` to the system:

!bc pycod
# bcs is a list of DirichletBC objects
for bc in bcs:
    bc.apply(A, b)
!ec

idx{`assemble_system`}

There is an alternative function `assemble_system`, which can
assemble the system and take boundary conditions into account in one call:

!bc pycod
A, b = assemble_system(a, L, bcs)
!ec
The `assemble_system` function incorporates the boundary conditions
in the element matrices and vectors, prior to assembly.
The conditions are also incorporated in a symmetric way to preserve
eventual symmetry of the coefficient matrix.
#That is, for each degree of freedom
#that is known, the corresponding row and column is zero'ed out and 1
#is placed on the main diagonal, and the right-hand side `b` is
#modified by subtracting the column in `A` times the value of the
#degree of, and then the corresponding entry in `b` is replaced by the
#known value of the degree of freedom.
With `bc.apply(A, b)` the
matrix `A` is modified in an nonsymmetric way.
#: The row is zero'ed out
#and 1 is placed on the main diagonal, and the degree of freedom value
#is inserted in `b`.

Note that the solution `u` is, as before, a `Function` object.
The degrees of freedom, $U=A^{-1}b$, are filled
into `u`'s `Vector` object (`u.vector()`)
by the `solve` function.

The object `A` is of type `Matrix`, while `b` and
`u.vector()` are of type `Vector`. We may convert the
matrix and vector data to `numpy` arrays by calling the
`array()` method as shown before. If you wonder how essential
boundary conditions are incorporated in the linear system, you can
print out `A` and `b` before and after the
`bc.apply(A, b)` call:

!bc pycod
A = assemble(a)
b = assemble(L)
if mesh.num_cells() < 16:  # print for small meshes only
    print(A.array())
    print(b.array())
bc.apply(A, b)
if mesh.num_cells() < 16:
    print(A.array())
    print(b.array())
!ec


With access to the elements in `A` through a `numpy` array we can easily
perform computations on this matrix, such as computing the eigenvalues
(using the `eig` function in `numpy.linalg`). We can alternatively dump
`A.array()` and `b.array()` to file in MATLAB format and invoke
MATLAB or Octave to analyze the linear system.
Dumping the arrays to MATLAB format is done by

!bc pycod
import scipy.io
scipy.io.savemat('Ab.mat', {'A': A.array(), 'b': b.array()})
!ec
Writing `load Ab.mat` in MATLAB or Octave will then make
the array variables `A` and `b` available for computations.

idx{SLEPc}

Matrix processing in Python or MATLAB/Octave is only feasible for
small PDE problems since the `numpy` arrays or matrices in MATLAB
file format are dense matrices. FEniCS also has an interface to the
eigensolver package SLEPc, which is a preferred tool for computing the
eigenvalues of large, sparse matrices of the type encountered in PDE
problems (see `demo/la/eigenvalue` in the FEniCS source code tree
for a demo).

By default, `solve(A, U, b)` applies sparse LU decomposition
as solver. Specification of an iterative solver and preconditioner
is done through two optional arguments:

!bc pycod
solve(A, U, b, 'cg', 'ilu')
!ec
Appropriate names of solvers and preconditioners are found in
Section ref{ftut:app:solver:prec}.

idx{KrylovSolver}

To control tolerances in the stopping criterion and the maximum
number of iterations, one can explicitly form a `KrylovSolver` object
and set items in its `parameters` attribute
(see also Section ref{ftut:poisson1:solver:problem}):

!bc pycod
solver = KrylovSolver('cg', 'ilu')
prm = solver.parameters
prm['absolute_tolerance'] = 1E-7
prm['relative_tolerance'] = 1E-4
prm['maximum_iterations'] = 1000
u = Function(V)
U = u.vector()
set_log_level(DEBUG)
solver.solve(A, U, b)
!ec
The function `solver_linalg` in the
program file `${prog["poisson_vc"]}.py` implements a solver function where
the user can choose between different types of assembly: the variational
(`solve(a == L, u, bc)`), assembling the matrix and right-hand side separately, and assembling the system such that the coefficient matrix preserves
symmetry.
The function `application_linalg` runs a test problem on sequence of
meshes and solves the problem with symmetric and non-symmetric modification
of the coefficient matrix. One can monitor the number of Krylov
method iteration and realize that with a symmetric coefficient matrix,
the Conjugate Gradient method requires slightly fewer iterations than
GMRES in the non-symmetric case. Taking into account that the Conjugate
Gradient method has less work per iteration, there is some efficiency to
be gained by using `assemble_system`.

[hpl: Running `application_linalg`, the results are strange: Why does
the `solve(a==L,...)` method need many more iterations than `solve(A,
U, b, ...)` when we use the same Krylov parameter settings? Something
wrong with the settings?]

idx{random start vector (linear systems)}

The choice of start vector for the iterations in a linear solver is often
important. With the `solver.solve(A, U, b)` call the default start vector
is the zero vector. A start vector
with random numbers in the interval $[-100,100]$ can be computed as

!bc pycod
n = u.vector().array().size
U = u.vector()
U[:] = numpy.random.uniform(-100, 100, n)
solver.parameters['nonzero_initial_guess'] = True
solver.solve(A, U, b)
!ec
Note that we must turn off the default behavior of setting the start
vector (``initial guess'') to zero, and then the provided value of `U`
is used as start vector.

Creating the linear system explicitly in a program can have some
advantages in more advanced problem settings. For example, $A$ may
be constant throughout a time-dependent simulation, so we can avoid
recalculating $A$ at every time level and save a significant amount
of simulation time.  Sections ref{ftut:timedep:diffusion1:impl}
and ref{ftut:timedep:diffusion1:noassemble} deal with this topic
in detail.


# In other problems, we may divide the variational
# problem and linear system into different terms, say $A=M + {\dt} K$,
# where $M$ is a matrix arising from a term like $\partial u/\partial t$,
# $K$ is a term corresponding to a Laplace operator, and $\dt$ is
# a time discretization parameter. When $\dt$ is changed in time,
# we can efficiently recompute $A = M + {\dt} K$ without
# reassembling the constant matrices $M$ and $K$. This strategy may
# speed up simulations significantly.



===== Taking advantage of structured mesh data =====
label{ftut:structviz}
idx{structured mesh}
idx{visualization, structured mesh}
idx{`scitools`}

When finite element computations are done on a structured rectangular
mesh, maybe with uniform partitioning, VTK-based tools for completely
unstructured 2D/3D meshes are not required.  Instead we can use
visualization and data analysis tools for *structured data*.
Such data typically appear in finite difference simulations and
image analysis.  Analysis and visualization of structured data are faster
and easier than doing the same with data on unstructured meshes, and
the collection of tools to choose among is much larger.  We shall
demonstrate the potential of such tools and how they allow for
tailored and flexible visualization and data analysis.

idx{`BoxField`}

A necessary first step is to transform our `mesh` object to an object
representing a rectangle with equally-shaped *rectangular* cells.  The
second step is to transform the one-dimensional array of nodal values
to a two-dimensional array holding the values at the corners of the
cells in the structured mesh. We want to access a value by its $i$ and
$j$ indices, $i$ counting cells in the $x$ direction, and $j$ counting
cells in the $y$ direction.  This transformation is in principle
straightforward, yet it frequently leads to obscure indexing errors,
so using software tools to ease the work is advantageous.

In the directory `src/modules`, associated with this booklet, we have
included a Python module `BoxField` that can take a finite element
function `u` computed by a FEniCS software and represent it on a
structured box-shaped mesh and assign or extract values by
multi-dimensional indexing: `[i]` in 1D, `[i,j]` in 2D, and `[i,j,k]`
in 3D. Given a finite element function `u`, the following function
returns a `BoxField` object that represents `u` on a structured mesh:

@@@CODE src/poisson_vc.py fromto: def structured_mesh@def application_structured_mesh
Note that we can only turn functions on meshes with P1 elements into
`BoxField` objects, so if `u` is based on another element type, we first
interpolate the scalar field onto a mesh with P1 elements. Also note
that to use the
function, we need to know the divisions into cells in the various
spatial directions (`divisions`).

The `u_box` object contains several useful data structures:

 * `u_box.grid`: object for the structured mesh
 * `u_box.grid.coor[X]`: grid coordinates in `X=0` direction
 * `u_box.grid.coor[Y]`: grid coordinates in `Y=1` direction
 * `u_box.grid.coor[Z]`: grid coordinates in `Z=2` direction
 * `u_box.grid.coorv[X]`: vectorized version of `u_box.grid.coor[X]`
   (for vectorized computations or surface plotting)
 * `u_box.grid.coorv[Y]`: vectorized version of `u_box.grid.coor[Y]`
 * `u_box.grid.coorv[Z]`: vectorized version of `u_box.grid.coor[Z]`
 * `u_box.values`: `numpy` array holding the `u` values;
   `u_box.values[i,j]` holds `u` at the mesh point with coordinates <linebreak>
   `(u_box.grid.coor[X], u_box.grid.coor[Y])`

=== Iterating over points and values ===

Let us go back to the `solver` function in the
`${prog["poisson_vc"]}.py` code from
Section ref{ftut:possion:2D:varcoeff}, compute `u`, map it onto a
`BoxField` object for a structured mesh representation, and
write out the coordinates and function values at all mesh points:

!bc pycod
u = solver(p, f, u0, nx, ny, 1, linear_solver='direct')
u_box = structured_mesh(u, (nx, ny))
u_ = u_box.values       # numpy array
X = 0;  Y = 1           # for indexing in x and y direction

# Iterate over 2D mesh points (i,j)
print('u_ is defined on a structured mesh with %s points' %
      str(u_.shape))
for j in range(u_.shape[1]):
    for i in range(u_.shape[0]):
        print('u[%d,%d]=u(%g,%g)=%g' %
              (i, j,
               u_box.grid.coor[X][i], u_box.grid.coor[X][j],
               u_[i,j]))
!ec

=== Finite difference approximations ===

Note that with `u_`, we can easily express finite difference approximation
of derivatives:

!bc pycod
x = u_box.grid.coor[X]
dx = x[1] - x[0]
u_xx = (u_[i-1,j] - 2*u_[i,j] + u_[i+1,j])/dx**2
!ec

idx{surface plot (structured mesh)}

=== Surface plot ===

The ability to access a finite element field in the way one can access
a finite difference-type of field is handy in many occasions, including
visualization and data analysis.
With Matplotlib we can create a surface plot, see
Figure ref{ftut:structviz:fig1} (upper left):

!bc pycod
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
fig = plt.figure()
ax = fig.gca(projection='3d')
cv = u_box.grid.coorv  # vectorized mesh coordinates
ax.plot_surface(cv[X], cv[Y], u_, cmap=cm.coolwarm,
                rstride=1, cstride=1)
plt.title('Surface plot of solution')
!ec
The key issue is to know that the coordinates needed for the surface
plot is in `u_box.grid.coorv` and that the values are in `u_`.

FIGURE: [fig/poisson_vc_structmesh2, width=800 frac=1] Various plots of the solution on a structured mesh. label{ftut:structviz:fig1}


idx{contour plot}

=== Contour plot ===

A contour plot can also be made by Matplotlib:

!bc pycod
fig = plt.figure()
ax = fig.gca()
levels = [1.5, 2.0, 2.5, 3.5]
cs = ax.contour(cv[X], cv[Y], u_, levels=levels)
plt.clabel(cs)  # add labels to contour lines
plt.axis('equal')
plt.title('Contour plot of solution')
!ec
The result appears in Figure ref{ftut:structviz:fig1} (upper right).


=== Curve plot through the mesh ===

A handy feature of `BoxField` objects is the ability to give a start
point in the grid and a direction, and then extract the field and
corresponding coordinates along the nearest line of mesh points. In 3D fields
one can also extract data in a plane.  Say we want to plot $u$ along
the line $y=0.4$. The mesh points, `x`, and the $u$ values
along this line, `u_val`, are extracted by

!bc pycod
start = (0, 0.4)
X = 0
x, u_val, y_fixed, snapped = u_box.gridline(start, direction=X)
!ec
The variable `snapped` is true if the line had to be snapped onto a
gridline and in that case `y_fixed` holds the snapped
(altered) $y$ value. To avoid interpolation in the structured mesh,
`snapped` is in fact *always* true.

A comparison of the numerical and exact solution along the line
$y=0.5$ (snapped from $y=0.4$) is made by the following code:

!bc pycod
start = (0, 0.4)
x, u_val, y_fixed, snapped = u_box.gridline(start, direction=X)
u_e_val = [u0((x_, y_fixed)) for x_ in x]

plt.figure()
plt.plot(x, u_val, 'r-')
plt.plot(x, u_e_val, 'bo')
plt.legend(['P1 elements', 'exact'], loc='upper left')
plt.title('Solution along line y=%g' % y_fixed)
plt.xlabel('x');  plt.ylabel('u')
!ec
See Figure ref{ftut:structviz:fig1} (lower left) for the resulting curve plot.

=== Curve plot of the flux ===

Let us also compare the numerical and
exact flux $-p\partial u/\partial x$ along the same line as above:

!bc pycod
flux_u = flux(u, p)
flux_u_x, flux_u_y = flux_u.split(deepcopy=True)

# Plot the numerical and exact flux along the same line
flux2_x = flux_u_x if flux_u_x.ufl_element().degree() == 1 \
          else interpolate(flux_x,
               FunctionSpace(u.function_space().mesh(),
                             'P', 1))
flux_u_x_box = structured_mesh(flux_u_x, (nx,ny))
x, flux_u_val, y_fixed, snapped = \
   flux_u_x_box.gridline(start, direction=X)
y = y_fixed

plt.figure()
plt.plot(x, flux_u_val, 'r-')
plt.plot(x, flux_u_x_exact(x, y_fixed), 'bo')
plt.legend(['P1 elements', 'exact'], loc='upper right')
plt.title('Flux along line y=%g' % y_fixed)
plt.xlabel('x');  plt.ylabel('u')
!ec
The second `plt.plot` command
requires a Python function `flux_u_x_exact(x,y)` to be
available for the exact flux expression.

Note that Matplotlib is one choice of plotting package. With the unified
interface in the "SciTools package": "https://github.com/hplgit/scitools" one
can access Matplotlib, Gnuplot, MATLAB, OpenDX, VisIt, and other plotting
engines through the same API.

idx{`sympy`}

=== Test problem ===

The graphics referred to in Figure ref{ftut:structviz:fig1} correspond to
a test problem with prescribed solution $\uex = H(x)H(y)$, where

!bt
\[ H(x) = e^{-16(x-\frac{1}{2})^2}\sin(3\pi x)\tp\]
!et
We just fit a function $f(x,y)$ in the PDE (can choose $p=1$),
and notice that $u=0$ along the
boundary of the unit square. Although it is easy to carry out the
differentiation of $f$ by hand and hardcode the resulting expressions
in an `Expression` object, a more reliable habit is to use Python's
symbolic computing engine, SymPy, to perform mathematics and
automatically turn formulas into C++ syntax for `Expression` objects.
A short introduction was given in Section ref{ftut:nonlinear:Newton:auto}.

We start out with defining the exact solution in `sympy`:

!bc pycod
from sympy import exp, sin, pi  # for use in math formulas
import sympy as sym
H = lambda x: exp(-16*(x-0.5)**2)*sin(3*pi*x)
x, y = sym.symbols('x[0], x[1]')
u = H(x)*H(y)
!ec

!bnotice Define symbolic coordinates as required in `Expression` objects
Note that we would normally write `x, y = sym.symbols('x y')`, but
if we want the resulting expressions to be have valid syntax for
`Expression` objects, and then $x$ reads `x[0]` and $y$ must be `x[1]`.
This is easily accomplished with `sympy` by defining the names of `x` and
`y` as `x[0]` and `x[1]`: `x, y = sym.symbols('x[0] x[1]')`.
!enotice

Turning the expression for `u` into C or C++ syntax for `Expression` objects
needs two steps. First we ask for the C code of the expression,

!bc pycod
u_c = sym.printing.ccode(u)
!ec
Printing out `u_c` gives (the output is here manually broken into two
lines):

!bc
-exp(-16*pow(x[0] - 0.5, 2) - 16*pow(x[1] - 0.5, 2))*
sin(3*M_PI*x[0])*sin(3*M_PI*x[1])
!ec
The necessary syntax adjustment is replacing
the symbol `M_PI` for $\pi$ in C/C++ by `pi` (or `DOLFIN_PI`):

!bc pycod
u_c = u_c.replace('M_PI', 'pi')
u0 = Expression(u_c)
!ec

Thereafter, we can progress with the computation of $f = -\nabla\cdot(p\nabla u)$:

!bc pycod
p = 1
f = sym.diff(-p*sym.diff(u, x), x) + sym.diff(-p*sym.diff(u, y), y)
f = sym.simplify(f)
f_c = sym.printing.ccode(f)
f_c = f_c.replace('M_PI', 'pi')
f = Expression(f_c)
!ec
We also need a Python function for the exact flux $-p\partial u/\partial x$:

!bc pycod
flux_u_x_exact = sym.lambdify([x, y], -p*sym.diff(u, x),
                              modules='numpy')
!ec
It remains to define `p = Constant(1)` and set `nx` and `ny` before calling
`solver` to compute the finite element solution of this problem.


#FIGURE: [fig/poisson_vc_structmesh, width=800 frac=1] Various plots of the solution on a structured mesh. label{ftut:structviz:fig1a}


# #ifdef EXTRA
It should be easy with the information above to transform a finite
element field over a uniform rectangular or box-shaped mesh to the
corresponding `BoxField` object and perform MATLAB-style
visualizations of the whole field or the field over planes or along
lines through the domain.  By the transformation to a regular grid we
have some more flexibility than what the `plot` command in FEniCS
offers. However, we remark
that comprehensive tools like VisIt, MayaVi2, or ParaView also have
the possibility for plotting fields along lines and extracting planes
in 3D geometries, though usually with less degree of control compared
to Gnuplot, MATLAB, and Matplotlib.  For example, in investigations of
numerical accuracy or numerical artifacts one is often interested in
studying curve plots where only the nodal values sampled. This is
straightforward with a structured mesh data structure, but more
difficult in visualization packages utilizing unstructured grids, as
hitting exactly then nodes when sampling a function along a line
through the grid might be non-trivial.
# #endif

!split
======= Postprocessing computations =======

[hpl: Need a little intro.]


===== Computing functionals =====
label{ftut:poisson1:functionals}
idx{functionals}

After the solution $u$ of a PDE is computed, we occasionally want to compute
functionals of $u$, for example,

!bt
\begin{equation}
{1\over2}||\nabla u||^2 \equiv {1\over2}\int_\Omega \nabla u\cdot \nabla u \dx,
label{ftut:poisson1:functionals:energy}
\end{equation}
!et
which often reflects some energy quantity.
Another frequently occurring functional is the error

!bt
\begin{equation}
||\uex-u|| = \left(\int_\Omega (\uex-u)^2 \dx\right)^{1/2},
label{ftut:poisson1:functionals:error}
\end{equation}
!et
where $\uex$ is the exact solution. The error is of particular
interest when studying convergence properties.  Sometimes the interest
concerns the flux out of a part $\Gamma$ of the boundary
$\partial\Omega$,

!bt
\begin{equation}
F = -\int_\Gamma p\nabla u\cdot\normalvec \ds,
label{ftut:poisson1:functionals:flux}
\end{equation}
!et
where $\normalvec$ is an outward unit normal at $\Gamma$ and $p$ is a
coefficient (see the problem in Section ref{ftut:possion:2D:varcoeff}
for a specific example).  All these functionals are easy to compute
with FEniCS, and this section describes how it can be done.

idx{energy functional}

=== Energy functional ===

The integrand of the energy functional
(ref{ftut:poisson1:functionals:energy}) is described in the UFL
language in the same manner as we describe weak forms:

!bc pycod
energy = 0.5*dot(grad(u), grad(u))*dx
E = assemble(energy)
!ec
The `assemble` call performs the integration.  It is possible to
restrict the integration to subdomains, or parts of the boundary, by
using a mesh function to mark the subdomains as explained in Section
ref{ftut:poisson:multi:bc}.

# #ifdef EXTRA
The function `energy` in `${prog["poisson_membrane"]}.py` carries out the computation of
the elastic energy

idx{`${prog["poisson_membrane"]}.py`}

[hpl: Things are right wrt scaling so far. The PDE is valid for
$\beta=0$ so use this case to check scaled energy expressions. Energy
is stress from a dimensional point of view: $\int
\sigma(\epsilon)d\epsilon = \int E\epsilon d\epsilon \sim E$. The
$\beta=0$ case corresponds to $A/(2\pi\sigma)$ constant on the
right-hand side in non-scaled coordinates. Must be something with
$\beta\rightarrow 0$, norm of $\nabla w$ goes to zero, no...not from
the exact solution from $w$.]

!bt
\begin{equation*}
{1\over2}||T\nabla D||^2 = {1\over2}\left(\frac{TD_c}{R}\right)^2R^2
||\bar\nabla w||^2 =
{1\over2}\left({AR^2\over 8\pi\sigma}\right)^2
||\nabla w||^2 = \hbox{ no beta! }
\left(\frac{A}{8\pi}\right)^2\beta^2||\nabla w||^2
\end{equation*}
!et
in the membrane problem from Section ref{ftut:poisson:membrane}.
# #endif

idx{error functional}

=== Error functional ===

Computation of (ref{ftut:poisson1:functionals:error}) is typically done
by

!bc pycod
error = (u - u_exact)**2*dx
E = sqrt(abs(assemble(error)))
!ec
The exact solution $\uex$ is here in a `Function` or `Expression`
object `u_exact`, while `u` is the finite element approximation.
(Sometimes, for very small error values, the result of
`assemble(error)` can be a (very small) negative number, so we have
used `abs` in the expression for `E` above to ensure a positive value
for the `sqrt` function.)

As will be explained and demonstrate in Section
ref{ftut:poisson1:convrates}, the integration of `(u - u_exact)**2*dx`
can result in too optimistic convergence rates unless one is careful
how `u_exact` is transferred onto a mesh. The general recommendation
for reliable error computation is to use the `errornorm` function (see
`pydoc fenics.errornorm` and Section ref{ftut:poisson1:convrates} for
more information):

!bc pycod
E = errornorm(u_exact, u)
!ec


idx{flux functional}

=== Flux Functionals ===

To compute flux integrals like $F = -\int_\Gamma p\nabla
u\cdot\normalvec \ds$ we need to define the $\normalvec$ vector,
referred to as *facet normal* in FEniCS. If the surface domain
$\Gamma$ in the flux integral is the complete boundary we can perform
the flux computation by

!bc pycod
n = FacetNormal(mesh)
flux = -p*dot(grad(u), n)*ds
total_flux = assemble(flux)
!ec
Although `grad(u)` and `grad(u)` are interchangeable in the above
expression when `u` is a scalar function, we have chosen to write
`grad(u)` because this is the right expression if we generalize the
underlying equation to a vector Laplace/Poisson PDE. With `grad(u)` we
must in that case write `dot(n, grad(u))`.

It is possible to restrict the integration to a part of the boundary
using a mesh function to mark the relevant part, as explained in
Section ref{ftut:poisson:multi:bc}. Assuming that the part corresponds
to subdomain number `i`, the relevant syntax for the variational
formulation of the flux is `-p*dot(grad(u), n)*ds(i)`.


===== Computing convergence rates =====
label{ftut:poisson1:convrates}

To illustrate error computations and convergence of finite element
solutions, we have included a function `convergence_rate` in
the `${prog["poisson_vc"]}.py` program. This is a tool that is very
handy when verifying finite element codes and will therefore be explained in
detail here.


The $L^2$ norm of the error in a finite element approximation $u$,
$\uex$ being the exact solution, is given by

=== Various ways of computing the error ===

!bt
\[ E = \left(\int_\Omega (u_e-u)^2 \dx\right)^{1/2},\]
!et
and implemented in FEniCS by

!bc pycod
error = (u - u_e)**2*dx
E = sqrt(abs(assemble(error)))
!ec
Sometimes, for very small error values, the result of
`assemble(error)` can be a (very small) negative number, so we have
used `abs` in the expression for `E` above to ensure a positive value
for the `sqrt` function.

We remark that `u_e` will, in the expression
above, be interpolated onto the function space `V` before `assemble`
can perform the integration over the domain. This implies that the
exact solution used in the integral will vary linearly over the cells,
and not as a sine function, if `V` corresponds to linear Lagrange
elements.  This situation may yield a smaller error `u - u_e` than
what is actually true.  More accurate representation of the exact
solution is easily achieved by interpolating the formula onto a space
defined by higher-order elements, say of third degree:

!bc pycod
Ve = FunctionSpace(mesh, 'P', degree=3)
u_e_Ve = interpolate(u_e, Ve)
error = (u - u_e_Ve)**2*dx
E = sqrt(assemble(error))
!ec
To achieve complete mathematical control of which function space the
computations are carried out in, we can explicitly interpolate `u` to
the same space:

!bc pycod
u_Ve = interpolate(u, Ve)
error = (u_Ve - u_e_Ve)**2*dx
!ec

The square in the expression for `error` will be expanded and lead to
a lot of terms that almost cancel when the error is small, with the
potential of introducing significant rounding errors.  The function
`errornorm` is available for avoiding this effect by first
interpolating `u` and `u_exact` to a space with higher-order elements,
then subtracting the degrees of freedom, and then performing the
integration of the error field. The usage is simple:

!bc pycod
E = errornorm(u_exact, u, normtype='L2', degree=3)
!ec
It is illustrative to look at the short implementation of `errornorm`:

!bc pycod
def errornorm(u_exact, u, Ve):
    u_Ve = interpolate(u, Ve)
    u_e_Ve = interpolate(u_exact, Ve)
    e_Ve = Function(Ve)
    # Subtract degrees of freedom for the error field
    e_Ve.vector()[:] = u_e_Ve.vector().array() - \
                       u_Ve.vector().array()
    error = e_Ve**2*dx
    return sqrt(assemble(error))
!ec
The `errornorm` procedure turns out to be identical to computing
the expression `(u_e - u)**2*dx` directly in
the present test case.

Sometimes it is of interest to compute the error of the
gradient field: $||\nabla (u-\uex)||$
(often referred to as the $H^1$ seminorm of the error).
Given the error field `e_Ve` above, we simply write

!bc pycod
H1seminorm = sqrt(assemble(dot(grad(e_Ve), grad(e_Ve))*dx))
!ec

All the various types of error computations here are placed in a
function `compute_errors` in `${prog["poisson_vc"]}.py`:
[hpl: Necessary to repeat code? New info is essentiall the return dict.]
[hpl: Anders, I (in 2010...) ran into problems with `fenics.errornorm`,
see comments in the code below, and made the version below. We should
check out these problems again and adjust `fenics.errornorm` if
necessary.]

@@@CODE src/poisson_vc.py fromto: def compute_errors@def convergence_rate

=== Computing convergence rates empirically ===

Calling the `solver` function for finer and finer meshes enables us to
study the convergence rate. Define the element size $h=1/n$, where $n$
is the number of cell divisions in $x$ and $y$ direction (`n=Nx=Ny` in
the code). We perform experiments with $h_0>h_1>h_2\cdots$ and compute
the corresponding errors $E_0, E_1, E_3$ and so forth.  Assuming
$E_i=Ch_i^r$ for unknown constants $C$ and $r$, we can compare two
consecutive experiments, $E_i=Ch_i^r$ and $E_{i-1}=Ch_{i-1}^r$, and
solve for $r$:

!bt
\begin{equation*}
r = {\ln(E_i/E_{i-1})\over\ln (h_i/h_{i-1})}\tp
\end{equation*}
!et
The $r$ values should approach the expected convergence
rate `degree+1` as $i$ increases.

The procedure above can easily be turned into Python code. Here
we run through a different types of elements (P1, P2, P3, and P4),
perform experiments over a series of refined meshes, and for
each experiment report the six error types as returned by `compute_errors`:

@@@CODE src/poisson_vc.py fromto: def convergence_rate@def structured_mesh
Note how we make a complete general function `convergence_rate`, aimed at
any 2D Poisson problem in the class we now can solve, and then call
this general function in `convergence_rate_sin` for a special test
case.

=== Test problem ===

Section ref{ftut:poisson:gradu} specifies a more complicated solution,

!bt
\begin{equation*}
u(x,y) = \sin(\omega\pi x)\sin(\omega\pi y)
\end{equation*}
!et
on the unit square.
This choice implies $f(x,y)=2\omega^2\pi^2 u(x,y)$.
With $\omega$ restricted to an integer
it follows that $u_0=0$.

We need to define the
appropriate boundary conditions, the exact solution, and the $f$ function
in the code:

!bc pycod
def boundary(x, on_boundary):
    return on_boundary

bc = DirichletBC(V, Constant(0.0), boundary)

omega = 1.0
u_e = Expression('sin(omega*pi*x[0])*sin(omega*pi*x[1])',
                 omega=omega)

f = 2*pi**2*omega**2*u_e
!ec

=== Experiments ===

Calling `convergence_rate_sin()` gives some interesting results.
Using the error measure `E5` based on the infinity norm of the
difference of the degrees of freedom, we have

|---c---------c---------c---------c---------c---------c-----|
| element | $n=8$   | $n=16$  | $n=32$  | $n=64$  | $n=128$ |
|---l---------r---------r---------r---------r---------r-----|
| P1      | 1.99    | 1.97    | 1.99    | 2.0     | 2.0     |
| P2      | 3.99    | 3.96    | 3.99    | 4.0     | 3.99    |
| P3      | 3.96    | 3.89    | 3.96    | 3.99    | 4.0     |
| P4      | 3.75    | 4.99    | 5.0     | 5.0     |         |
|-----------------------------------------------------------|

The computations with P4 elements on a $128\times 128$ with a
direct solver (UMFPACK) on a small laptop broke down.
Otherwise we achieve expected results: the error goes like
$h^{d+1}$ for elements of degree $d$. Also $L^2$ norms based
on the `errornorm` gives the expected $h^{d+1}$ rate for
$u$ and $h^d$ for $\nabla u$.

However, using `(u - u_exact)**2` for the error computation, which implies
interpolating `u_exact` onto the same space as `u`, results in $h^4$
convergence for P2 elements.

|---c---------c---------c---------c---------c---------c-----|
| element | $n=8$   | $n=16$  | $n=32$  | $n=64$  | $n=128$ |
|---l---------r---------r---------r---------r---------r-----|
| P1      | 1.98    | 1.94    | 1.98    | 2.0     | 2.0     |
| P2      | 3.98    | 3.95    | 3.99    | 3.99    | 3.99    |
| P3      | 3.69    | 4.03    | 4.01    | 3.95    | 2.77    |
|-----------------------------------------------------------|

This is an example where it is important to interpolate `u_exact` to a
higher-order space (polynomials of degree 3 are sufficient here) to
avoid computing a too optimistic convergence rate.

# Problems with interpolate(u,Ve) - interpolate(u_exact,Ve) for
# high degree and large meshes. Rounding errors? errornorm is the
# remedy?
# interpolate(u,Ve) - interpolate(u_exact,Ve)
# P1: 1.98, 1.96, 1.99, 2.0, 2.0
# P2: 3.01, 3.03, 3.01, 3.0, 3.02
# P3: 2.7, 4.02, 4.0, 2.63, 0.17
# P4: 1.54, 5.11, 0.91, 0.15, -0.01


Checking convergence rates is the next best method for verifying PDE codes
(the best being a numerical solution without approximation errors
as in Section ref{ftut:poisson1:verify1} and many other places in this tutorial).

!split
======= Multiple domains and boundaries =======

[hpl: Need a little intro.]


===== Combining Dirichlet and Neumann conditions =====
label{ftut:poisson1:DN}

Let us make a slight extension of our two-dimensional Poisson problem
from Section ref{ftut:poisson1:bvp} and add a Neumann boundary
condition. The domain is still the unit square, but now we set the
Dirichlet condition $u=u_0$ at the left and right sides, $x=0$ and
$x=1$, while the Neumann condition

!bt
\begin{equation*}
-{\partial u\over\partial n}=g
\end{equation*}
!et
is applied to the remaining
sides $y=0$ and $y=1$.
The Neumann condition is also known as a *natural boundary condition*
(in contrast to an essential boundary condition).

idx{Neumann boundary conditions}

=== PDE problem ===

Let $\Gamma_D$ and $\Gamma_N$ denote the parts of $\partial\Omega$
where the Dirichlet and Neumann conditions apply, respectively.  The
complete boundary-value problem can be written as

!bt
\begin{align}
    - \nabla^2 u &= f \mbox{ in } \Omega,  \\
    u &= u_0 \mbox{ on } \Gamma_D,       \\
    - {\partial u\over\partial n} &= g \mbox{ on } \Gamma_N  \tp
\end{align}
!et
Again we choose $u=1+x^2 + 2y^2$ as the exact solution and adjust $f$, $g$, and
$u_0$ accordingly:

!bt
\begin{align*}
f &= -6,\\
g &= \left\lbrace\begin{array}{ll}
-4, & y=1\\
0,  & y=0
\end{array}\right.\\
u_0 &= 1 + x^2 + 2y^2\tp
\end{align*}
!et
For ease of programming we may introduce a $g$ function defined over the whole
of $\Omega$ such that $g$ takes on the right values at $y=0$ and
$y=1$. One possible extension is

!bt
\begin{equation*}
g(x,y) = -4y\tp
\end{equation*}
!et

=== Variational formulation ===

The first task is to derive the variational problem. This time we cannot
omit the boundary term arising from the integration by parts, because
$v$ is only zero on $\Gamma_D$. We have

!bt
\begin{equation*}
 -\int_\Omega (\nabla^2 u)v \dx
= \int_\Omega\nabla u\cdot\nabla v \dx - \int_{\partial\Omega}{\partial u\over
\partial n}v \ds,
\end{equation*}
!et
and since $v=0$ on $\Gamma_D$,

!bt
\begin{equation*}
- \int_{\partial\Omega}{\partial u\over
\partial n}v \ds
=
- \int_{\Gamma_N}{\partial u\over
\partial n}v \ds
= \int_{\Gamma_N}gv \ds,
\end{equation*}
!et
by applying the boundary condition on $\Gamma_N$.
The resulting weak form reads

!bt
\begin{equation}
\int_{\Omega} \nabla u \cdot \nabla v \dx +
\int_{\Gamma_N} gv \ds
= \int_{\Omega} fv \dx\tp
label{ftut:poisson:2D:DN:weak}
\end{equation}
!et
Expressing this equation
in the standard notation $a(u,v)=L(v)$ is straightforward with

!bt
\begin{align}
a(u, v) &= \int_{\Omega} \nabla u \cdot \nabla v \dx,
label{ftut:poisson2:vard:a}\\
L(v) &= \int_{\Omega} fv \dx -
\int_{\Gamma_N} gv \ds\tp  label{ftut:poisson2:vard:L}
\end{align}
!et

=== Implementation ===

How does the Neumann condition impact the implementation?
Let us go back to the very simplest file,
`${prog["poisson_flat"]}.py`, from
Section ref{ftut:poisson1:impl:code},
we realize that the statements remain almost the same.
Only two adjustments are necessary:

  * The function describing the boundary where Dirichlet conditions
    apply must be modified.
  * The new boundary term must be added to the expression in `L`.

The first adjustment can be coded as

!bc pycod
def Dirichlet_boundary(x, on_boundary):
    if on_boundary:
        if x[0] == 0 or x[0] == 1:
            return True
        else:
            return False
    else:
        return False
!ec
A more compact implementation reads

!bc pycod
def Dirichlet_boundary(x, on_boundary):
    return on_boundary and (x[0] == 0 or x[0] == 1)
!ec

!bwarning Never use `==` for comparing real numbers!
A list like `x[0] == 1` should never be used if `x[0]` is a real number,
because rounding errors in `x[0]` may make the test fail even when it is
mathematically correct. Consider

!bc pycod
>>> 0.1 + 0.2 == 0.3
False
>>> 0.1 + 0.2
0.30000000000000004
!ec

Comparison of real numbers need to use tolerances! The values of the
tolerances depend on the size of the numbers involved in arithmetic
operations:

!bc pycod
>>> abs(0.1+0.2 - 0.3)
5.551115123125783e-17
>>> abs(1.1+1.2 - 2.3)
0.0
>>> abs(10.1+10.2 - 20.3)
3.552713678800501e-15
>>> abs(100.1+100.2 - 200.3)
0.0
>>> abs(1000.1+1000.2 - 2000.3)
2.2737367544323206e-13
>>> abs(10000.1+10000.2 - 20000.3)
3.637978807091713e-12
!ec
For numbers around unity, tolerances as low as $3\cdot 10^{-16}$ can be used
(in fact, this tolerance is known as the constant `DOLFIN_EPS` in FEniCS),
otherwise an appropriate tolerance must be found.

Testing for `x[0] == 1` should therefore be implemented as

!bc pycod
tol = 1E-14
if abs(x[0] - 1) < tol:
    ...
!ec
!ewarning

Here is a new boundary function using tolerances in the test:

!bc pycod
def Dirichlet_boundary(x, on_boundary):
    tol = 1E-14   # tolerance for coordinate comparisons
    return on_boundary and \
           (abs(x[0]) < tol or abs(x[0] - 1) < tol)
!ec

The second adjustment of our program concerns the definition of `L`,
where we have to add a boundary integral and a definition of the $g$
function to be integrated:

!bc pycod
g = Expression('-4*x[1]')
L = f*v*dx - g*v*ds
!ec
The `ds` variable implies a boundary integral, while `dx`
implies an integral over the domain $\Omega$.
No more modifications are necessary.


===== Multiple Dirichlet conditions =====
label{ftut:poisson:multiple:Dirichlet}

The PDE problem from the previous section applies a function $u_0(x,y)$
for setting Dirichlet conditions at two parts of the boundary.
Having a single function to set multiple Dirichlet conditions is
seldom possible. The more general case is to have $m$ functions for
setting Dirichlet conditions on $m$ parts of the boundary.
The purpose of this section is to explain how such multiple conditions
are treated in FEniCS programs.

Let us return to the case from Section ref{ftut:poisson1:DN} and define
two separate functions for the two Dirichlet conditions:

!bt
\begin{align*}
    - \nabla^2 u &= -6 \mbox{ in } \Omega, \\
    u &= u_L \mbox{ on } \Gamma_0, \\
    u &= u_R \mbox{ on } \Gamma_1, \\
    - {\partial u\over\partial n} &= g \mbox{ on } \Gamma_N \tp
\end{align*}
!et
Here, $\Gamma_0$ is the boundary $x=0$, while $\Gamma_1$ corresponds
to the boundary $x=1$.  We have that $u_L = 1 + 2y^2$, $u_R = 2 +
2y^2$, and $g=-4y$.

=== Functions for marking Dirichlet boundaries ===

For the left boundary $\Gamma_0$ we define the
usual triple of a function for the boundary value, a function for
defining the boundary of interest, and a `DirichletBC` object:

!bc pycod
u_L = Expression('1 + 2*x[1]*x[1]')

def left_boundary(x, on_boundary):
    tol = 1E-14   # tolerance for coordinate comparisons
    return on_boundary and abs(x[0]) < tol

Gamma_0 = DirichletBC(V, u_L, left_boundary)
!ec
For the boundary $x=1$ we write a similar code snippet:

!bc pycod
u_R = Expression('2 + 2*x[1]*x[1]')

def right_boundary(x, on_boundary):
    tol = 1E-14   # tolerance for coordinate comparisons
    return on_boundary and abs(x[0] - 1) < tol

Gamma_1 = DirichletBC(V, u_R, right_boundary)
!ec
The various essential conditions are then collected in a list
and used in the solution process:

!bc pycod
bcs = [Gamma_0, Gamma_1]
...
solve(a == L, u, bcs)
# or
problem = LinearVariationalProblem(a, L, u, bcs)
solver  = LinearVariationalSolver(problem)
solver.solve()
!ec

In other problems, where the $u$ values are constant at a part of the
boundary, we may use a simple `Constant` object instead of an
`Expression` object.

# #ifdef EXTRA
# We come to this in a minute with internal subdomains...

=== Classes for marking Dirichlet boundaries ===

Instead of using a function like `left_boundary(x, on_boundary)` to
mark a boundary, we can alternatively use a class, which allows
for more flexibility in more complicated problems. The class for marking
a boundary is derived from class `SubDomain` and has a method `inside(self, x, on_boundary)` for the code that returns whether the `point` is on the
boundary in question or not. Our previous `left_boundary` function
takes this form in its class version:

!bc pycod
class LeftBoundary(SubDomain):
    def inside(self, x, on_boundary):
        tol = 1E-14   # tolerance for coordinate comparisons
        return on_boundary and abs(x[0]) < tol

left_boundary = LeftBoundary()
Gamma_0 = DirichletBC(V, u_L, left_boundary)
!ec
# #endif

===== Working with subdomains =====
label{ftut:possion:2D:2mat:impl}

idx{heterogeneous media}
idx{multi-material domain}


Solving PDEs in domains made up of different materials is a frequently
encountered task. In FEniCS, these kind of problems are handled by
defining subdomains inside the domain. The subdomains may represent
the various materials. We can thereafter define material properties
through functions, known in FEniCS as *mesh functions*, that are
piecewise constant in each subdomain.  A simple example with two
materials (subdomains) in 2D will demonstrate the basic steps in the
process.

FIGURE: [fig/layered_medium_2, width=400 frac=0.5] Medium with discontinuous material properties. label{ftut:possion:2D:2mat:fig1}

Suppose we want to solve

!bt
\begin{equation} label{ftut:poisson:2D:2mat:varcoeff2}
    \nabla\cdot \left\lbrack k(x,y)\nabla u(x,y)\right\rbrack = 0,
\end{equation}
!et
in a domain $\Omega$ consisting of two subdomains where $k$ takes on
a different value in each subdomain.
For simplicity, yet without loss of generality, we choose for the current
implementation
the domain $\Omega = [0,1]\times [0,1]$ and divide it into two equal
subdomains,
as depicted in Figure ref{ftut:possion:2D:2mat:fig1},

!bt
\begin{equation*}
\Omega_0 = [0, 1]\times [0,1/2],\quad
\Omega_1 = [0, 1]\times (1/2,1]\tp
\end{equation*}
!et
We define $k(x,y)=k_0$ in $\Omega_0$ and $k(x,y)=k_1$ in $\Omega_1$,
where $k_0>0$ and $k_1>0$ are given constants.

Physically, the present problem may correspond to heat conduction, where
the heat conduction in $\Omega_1$ is more efficient than
in $\Omega_0$. An alternative interpretation is flow in porous media
with two geological layers, where the layers' ability to transport
the fluid differ.

=== Expression objects with if test ===

The simplest way of implementing a variable $k$ is to define an
`Expression` object where we return the appropriate $k$ value
depending on the position in space.  Since we need some testing on the
coordinates, the most straightforward approach is to define a subclass
of `Expression`, where we can use a full Python method instead of just
a C++ string formula for specifying a function.  The method that
defines the function is called `eval`:

!bc pycod
class K(Expression):
    def set_k_values(self, k0, k1):
        self.k0, self.k1 = k0, k1

    def eval(self, value, x):
        """x: spatial point, value[0]: function value."""
	# Fill in-place value[0] for scalar function,
	# value[:] for vector function (no return)

	tol = 1E-14  # Tolerance for coordinate comparisons
        if x[1] <= 0.5+tol:
	    value[0] = self.k0
	else:
	    value[0] = self.k1

# Initialize
k = K()
k.set_k_values(1, 0.01)
!ec
The `eval` method gives great flexibility in defining functions, but a
downside is that C++ calls up `eval` in Python for each point `x`,
which is a slow process, and the number of calls is proportional to
the number of numerical integration points in the mesh (about the
number of degrees of freedom).  Function expressions in terms of
strings are compiled to efficient C++ functions, being called from
C++, so we should try to express functions as string expressions if
possible. (The `eval` method can also be defined through C++ code, but
this is much more complicated and not covered here.)  The idea is to
use inline if tests in C++:

!bc pycod
tol = 1E-14
k0 = 1.0
k1 = 0.01
k = Expression('x[1] <= 0.5+tol? k0 : k1',
               tol=tol, k0=k0, k1=k1)
!ec
The method with if tests on the location is feasible when the
subdomains have very simple shapes. A completely general method,
utilizing *mesh functions*, is described next.


idx{boundary specification (class)}

=== Mesh functions ===

We now address how to specify the subdomains $\Omega_0$ and $\Omega_1$
so that the method also works for subdomains of any shape. For this
purpose we need to use subclasses of class `SubDomain`, not only plain
functions as we have used so far for specifying boundaries. Consider
the boundary function

!bc pycod
def boundary(x, on_boundary):
    tol = 1E-14
    return on_boundary and abs(x[0]) < tol
!ec
for defining the boundary $x=0$. Instead of using such a stand-alone
function, we can create an instance (or object)
of a subclass of `SubDomain`,
which implements the `inside` method as an alternative to the
`boundary` function:

!bc pycod
class Boundary(SubDomain):
    def inside(self, x, on_boundary):
        tol = 1E-14
        return on_boundary and abs(x[0]) < tol

boundary = Boundary()
bc = DirichletBC(V, Constant(0), boundary)
!ec
A word about computer science terminology may be used here: The term
*instance* means a Python object of a particular type (such as
`SubDomain`, `Function`, `FunctionSpace`, etc.).  Many use *instance*
and *object* as interchangeable terms. In other computer programming
languages one may also use the term *variable* for the same thing.  We
mostly use the well-known term *object* in this text.

A subclass of `SubDomain` with an `inside` method offers functionality
for marking parts of the domain or the boundary. Now we need to define
one class for the subdomain $\Omega_0$ where $y\leq 1/2$ and another
for the subdomain $\Omega_1$ where $y\geq 1/2$:

!bc pycod
tol = 1E-14  # Tolerance for coordinate comparisons

class Omega0(SubDomain):
    def inside(self, x, on_boundary):
        return x[1] <= 0.5+tol

class Omega1(SubDomain):
    def inside(self, x, on_boundary):
        return x[1] >= 0.5-tol
!ec
Notice the use of `<=` and `>=` in both tests. For a cell to belong
to, e.g., $\Omega_1$, the `inside` method must return `True` for all
the vertices `x` of the cell. So to make the cells at the internal
boundary $y=1/2$ belong to $\Omega_1$, we need the test `x[1] >=
0.5`. However, because of potential rounding errors in the coordinates
`x[1]`, we use a tolerance in the comparisons: `x[1] >= 0.5-tol`.

The next task is to use a *mesh function* to mark all cells in
$\Omega_0$ with the subdomain number 0 and all cells in $\Omega_1$
with the subdomain number 1.  Our convention is to number subdomains
as $0,1,2,\ldots$.

A `MeshFunction` object is a discrete function that can be evaluated
at a set of so-called *mesh entities*. Examples of mesh entities are
cells, facets, and vertices. A `MeshFunction` over cells is suitable
to represent subdomains (materials), while a `MeshFunction` over
facets is used to represent pieces of external or internal boundaries.
Mesh functions over vertices can be used to describe continuous
fields.  The specialized classes `CellFunction` and `FacetFunction`
are used to construct mesh functions of cells and facets,
respectively.

Since we need to define subdomains of $\Omega$ in the present example,
we make use of a `CellFunction`. The constructor
is fed with two arguments: 1) the type of value: `'int'` for integers,
`'uint'` for positive (unsigned) integers, `'double'` for real
numbers, and `'bool'` for logical values; 2) a `Mesh` object.
Alternatively, the constructor can take just a filename and initialize
the `CellFunction` from data in a file.
#  #ifdef BOOK
We shall
demonstrate the file functionality in the next multi-material problem
in Section ref{ftut:possion:nD:nmat}.
#  #endif

We start with creating a `CellFunction` whose values are non-negative
integers (`'uint'`) for numbering the subdomains.
The appropriate code for two subdomains then reads

!bc pycod
materials = CellFunction('size_t', mesh)
# Mark subdomains with numbers 0 and 1
subdomain0 = Omega0()
subdomain0.mark(materials, 0)
subdomain1 = Omega1()
subdomain1.mark(materials, 1)

# Alternative
materials.set_all(0)
subdomain1.mark(materials, 1)
!ec

Calling `materials.array()` returns a `numpy` array of the
subdomain values. That is, `materials.array()[i]` is
the subdomain value of cell number `i`. This array is used to
look up the subdomain or material number of a specific element.

We need a function `k` that is constant in each subdomain $\Omega_0$
and $\Omega_1$. Since we want `k` to be a finite element function, it
is natural to choose a space of functions that is constant over each
element.  The family of discontinuous Galerkin methods, in FEniCS
denoted by `'DG'`, is suitable for this purpose. Since we want
functions that are piecewise constant, the value of the degree
parameter is zero:

!bc pycod
V0 = FunctionSpace(mesh, 'DG', 0)
k  = Function(V0)
!ec
To fill `k` with the right values in each element, we loop over
all cells (i.e., indices in `materials.array()`),
extract the corresponding subdomain number of a cell,
and assign the corresponding $k$ value to the `k.vector()` array:

!bc pycod
k_values = [1.5, 50]  # values of k in the two subdomains
for cell_no in range(len(materials.array())):
    material_no = materials.array()[cell_no]
    k.vector()[cell_no] = k_values[material_no]
!ec

Long loops in Python are known to be slow, so for large meshes
it is preferable to avoid such loops and instead use *vectorized code*.
Normally this implies that the loop must be replaced by
calls to functions from the `numpy` library that operate on complete
arrays (in efficient C code). The functionality we want in the present
case is to compute an array of the same size as
`materials.array()`, but where the value `i` of an entry
in `materials.array()` is replaced by `k_values[i]`.
Such an operation is carried out by the `numpy` function `choose`:

!bc pycod
help = numpy.asarray(materials.array(), dtype=numpy.int32)
k.vector()[:] = numpy.choose(help, k_values)
!ec
The `help` array is required since `choose` cannot work with
`materials.array()` because this array has elements of
type `uint32`. We must therefore transform this array to an array
`help` with standard `int32` integers.

The next section exemplifies a complete solver with a piecewise
constant coefficient, like $k$, defined through `SubDomain` objects,
combined with different types of boundary conditions.

idx{`CompiledSubDomain`}

=== C++ strings for subdomain definitions ===

The `SubDomain` class in Python is convenient, but leads to lots of
function calls from C++ to Python, which are slow. In large problems,
the subdomains should be defined through C++ code. This is easy to achieve
using the `CompiledSubDomain` object. Consider the definition of
classes `Omega0` and `Omega1` above in Python.
The key strings that define these subdomain can be expressed in
C++ syntax and fed to `CompiledSubDomain` as follows:

!bc pycod
tol = 1E-14  # Tolerance for coordinate comparisons

subdomain0 = CompiledSubDomain(
                'x[1] <= boundary+tol', tol=1E-14, boundary=0.5)
subdomain1 = CompiledSubDomain(
                'x[1] >= boundary-tol', tol=1E-14, boundary=0.5)
!ec
As seen, one can have parameters in the strings and specify their
values by keyword arguments.
The resulting objects, `subdomain0` and `subdomain1`, can be used
as ordinary `SubDomain` objects.

Compiled subdomain strings can be applied for specifying boundaries as
well, e.g.,

!bc pycod
y_R = CompiledSubDomain('on_boundary && near(x[1], R, eps=tol)',
                        tol=1E-14, R=2)   # y=2
!ec

It is possible to feed the C++ string (without parameters) directly as
the third argument to `DirichletBC` without explicitly constructing a
`CompiledSubDomain` object:

!bc pycod
bc1 = DirichletBC(V, value, 'on_boundary && near(x[1], 2, 1E-14)')
!ec

idx{`near`}

% if EXV:
===== Exercise: Efficiency of Python vs C++ expressions =====
label{ftut:poisson:exer:eff:expression}
file=Expression_efficiency

Consider a cube mesh with $N$ cells in each spatial direction.
We want to define a `Function` on this mesh where the
values are given by the mathematical function $f(x,y,z)=a\sin(bxyz)$,
where $a$ and $b$ are two parameters. Write a `class SineXYZ`:

!bc pycod
class SineXYZ(Expression):
    def set_parameters(self, a, b):
        self.a, self.b = a, b

    def eval(self, value, x):
        value[0] = self.a*sin(self.b*x[0]*x[1]*x[2])
!ec
Create an alternative `Expression` based on giving the formula for $f(x,y,z)$
as a C++ code string. Compare the computational efficiency of the
two implementations (e.g., using `time.clock()` to measure the CPU time).

The `sin` function used in class `SineXYZ.eval` can mean many things.
This is an advanced FEniCS function if imported from `fenics`.
Much more efficient versions for sin of numbers are found in `math.sin`
and `numpy.sin`. Compare the use `sin` from `fenics`, `math`, `numpy`, and
`sympy` (note that `sin` from `sympy` is very slow).

!bsol
Here is an appropriate program:

@@@CODE exer/Expression_efficiency.py
Running the program shows that `sin` from `math` is the most efficient choice,
but a string C++ runs 40 times faster. Note that `fenics.sin`, which is a
sine function in the UFL language that can work with symbolic expressions
in finite element forms, is (naturally) less efficient than the `sin`
functions for numbers in `math` and `numpy`.
!esol
% endif

===== Multiple Neumann, Robin, and Dirichlet condition =====
label{ftut:poisson:multi:bc}
idx{Dirichlet boundary conditions}
idx{Neumann boundary conditions}
idx{Robin boundary conditions}
idx{boundary conditions}

Consider the model problem from Section
ref{ftut:poisson:multiple:Dirichlet} where we had both Dirichlet and
Neumann conditions.  The term `v*g*ds` in the expression for `L`
implies a boundary integral over the complete boundary, or in FEniCS
terms, an integral over all exterior facets.  However, the
contributions from the parts of the boundary where we have Dirichlet
conditions are erased when the linear system is modified by the
Dirichlet conditions.  We would like, from an efficiency point of
view, to integrate `v*g*ds` only over the parts of the boundary where
we actually have Neumann conditions.  And more importantly, in other
problems one may have different Neumann conditions or other conditions
like the Robin type condition.  With the mesh function concept we can
mark different parts of the boundary and integrate over specific
parts.  The same concept can also be used to treat multiple Dirichlet
conditions.  The forthcoming text illustrates how this is done.

=== Three types of boundary conditions ===

We extend our repertoire of boundary conditions to three types:
Dirichlet, Neumann, and Robin.  Dirichlet conditions apply to some
parts $\Gamma_{D,0}$, $\Gamma_{D,1}$, $...$, of the boundary:

!bt
\[ u_{0,0}\hbox{ on }\Gamma_{D,0},\quad
u_{0,1}\hbox{ on }\Gamma_{D,1}, \ldots\]
!et
where $u_{0,i}$ are prescribed functions, $i=0,1,\ldots$
On other parts, $\Gamma_{N,0}$, $\Gamma_{N,1}$, and so on, we have
Neumann conditions

!bt
\[ -p{\partial u\over\partial n} = g_{0}\hbox{ on }\Gamma_{N,0},\quad
-p{\partial u\over\partial n} = g_{1}\hbox{ on }\Gamma_{N,1},\quad \ldots
\]
!et
Finally, we have *Robin conditions*

!bt
\begin{equation*}
-p{\partial u\over\partial n} = r(u-s),
label{ftut:poisson:multi:bc:Robin}
\end{equation*}
!et
where $r$ and $s$ are specified functions.  The Robin condition is
most often used to model heat transfer to the surroundings and arise
naturally from Newton's cooling law. In that case, $r$ is a heat
transfer coefficient, and $s$ is the temperature of the
surroundings. Both can be space and time-dependent.
The Robin conditions apply
at some parts $\Gamma_{R,0}$, $\Gamma_{R,1}$, and so forth:

!bt
\[ -p{\partial u\over\partial n} = r_0(u-s_0)\hbox{ on }\Gamma_{R,0},\quad
-p{\partial u\over\partial n} = r_1(u-s_1)\hbox{ on }\Gamma_{R,1},\quad \ldots
\]
!et

idx{Robin condition}


=== A general model problem ===

With the notation above,
the model problem to be solved with multiple Dirichlet, Neumann, and
Robin conditions can formally be defined as

!bt
\begin{align}
-\nabla\cdot(p\nabla u) &= -f, \mbox{ in } \Omega, label{ftut:poisson:2D:DN3}\\
u &= u_{0,i} \mbox{ on } \Gamma_{D,i},\quad i=0,1,\ldots
label{ftut:poisson:2D:DN3:bcD}\\
-p{\partial u\over\partial n} &= g_i \mbox{ on } \Gamma_{N,i},\quad
i=0,1,\ldots
label{ftut:poisson:2D:DN3:bcN}\\
-p{\partial u\over\partial n} &= r_i(u-s_i) \mbox{ on } \Gamma_{R,i},\quad
i=0,1,\ldots
label{ftut:poisson:2D:DN3:bcR}
\end{align}
!et

=== Variational formulation ===

Integration by parts of $-\int_\Omega v\nabla\cdot(p\nabla u) \dx$ becomes
as usual

!bt
\begin{equation*}
 -\int_\Omega v\nabla\cdot(p\nabla u) \dx
= \int_\Omega p\nabla u\cdot \nabla v \dx -
\int_{\partial\Omega}p\frac{\partial u}{\partial n}v \ds\tp
\end{equation*}
!et
The boundary integral does not apply to the parts of
the boundary where we have Dirichlet conditions ($\Gamma_{D,i}$).
Moreover, on the remaining parts, we must split the boundary integral
into the parts where we have Neumann and Robin conditions such that we
insert the right conditions as integrands.
Specifically, we have

!bt
\begin{align*}
-\int_{\partial\Omega}p\frac{\partial u}{\partial n}v \ds
&=
-\sum_i\int_{\Gamma_{N,i}}p\frac{\partial u}{\partial n} \ds
-\sum_i\int_{\Gamma_{R,i}}p\frac{\partial u}{\partial n} \ds\\
&=
\sum_i\int_{\Gamma_{N,i}}g_i \ds +
\sum_i\int_{\Gamma_{R,i}}r_i(u-s_i) \ds\tp
\end{align*}
!et
The variational formulation then becomes

!bt
\begin{equation}
F = \int_{\Omega} p\nabla u\cdot \nabla v \dx +
\sum_i\int_{\Gamma_{N,i}} g_iv \ds +
\sum_i\int_{\Gamma_{R,i}}r_i(u-s_i)v \ds
- \int_{\Omega} fv \dx =0\tp
label{ftut:poisson:multi:bc:varform}
\end{equation}
!et

We have been used to writing
this variational formulation in the standard notation
$a(u,v)=L(v)$, which requires that we identify all integrals with
*both* $u$ and $v$, and collect these in $a(u,v)$, while the remaining
integrals with $v$ and not $u$ go into $L(v)$.  The integral from the
Robin condition must of this reason be split in two parts:

!bt
\begin{equation*}
\int_{\Gamma_{R,i}}r_i(u-s_i)v \ds
= \int_{\Gamma_{R,i}} r_iuv \ds - \int_{\Gamma_{R,i}}r_is_iv \ds\tp
\end{equation*}
!et
We then have

!bt
\begin{align}
a(u, v) &= \int_{\Omega} p\nabla u\cdot \nabla v \dx
+ \sum_i\int_{\Gamma_{R,i}}r_iuv \ds,
label{ftut:poisson:2D:DN3:var:a}\\
L(v) &= \int_{\Omega} fv \dx -
\sum_i\int_{\Gamma_{N,i}} g_i v \ds + \sum_i\int_{\Gamma_{R,i}}r_is_iv \ds\tp
label{ftut:poisson:2D:DN3:var:L}
\end{align}
!et


## /usr/share/fenics/demo/documented/subdomains/python/demo_subdomains.py
## contains a good example, followed up in stokes solvers

=== Implementation of boundary conditions ===

Looking at our previous `solver` functions for solving the 2D Poisson equation,
the following new aspects must be taken care of:

  o definition of a mesh function over the boundary,
  o marking each side as a subdomain, using the mesh function,
  o splitting a boundary integral into parts.

A general approach to the first task is to mark each of the desired
boundaries with markers 0, 1, 2, and so forth. Here we aim at
the four sides of the unit square, marked with
0 ($x=0$), 1 ($x=1$), 2 ($y=0$), and 3 ($y=1$).
The marking of boundaries makes use of a mesh function object, but contrary to
Section ref{ftut:possion:2D:2mat:impl}, this is not a function over
cells, but a function over cell facets. We apply the `FacetFunction`
for this purpose:

!bc pycod
boundary_parts = FacetFunction('size_t', mesh)
!ec
As in Section ref{ftut:possion:2D:2mat:impl} we use a subclass of
`SubDomain` to identify the various parts of the mesh
function. Problems with domains of more complicated geometries may set
the mesh function for marking boundaries as part of the mesh
generation.  In our case, the $x=0$ boundary can be marked by

!bc pycod
class BoundaryX0(SubDomain):
    def inside(self, x, on_boundary):
        return on_boundary and abs(x[0]) < tol

bx0 = BoundaryX0()
bx0.mark(boundary_parts, 0)
!ec
Similarly, we make the classes `BoundaryX1` for the $x=1$ boundary,
`BoundaryY0` for the $y=0$ boundary, and `BoundaryY1` for the $y=1$
boundary, and mark these as subdomains 1, 2, and 3, respectively.

For generality of the implementation, we let the user specify
what kind of boundary condition that applies to each of the four
boundaries. We set up a Python dictionary for this purpose, with
the key as subdomain number and the value as a dictionary specifying
the kind of condition as key and a function as its value.
For example,

!bc
boundary_conditions = {
  0: {'Dirichlet': u0},
  1: {'Robin': (r, s)},
  2: {'Neumann: g}},
  3: {'Neumann', 0}}
!ec
specifies

 * a Dirichlet condition, with values implemented by an `Expression`
   or `Constant` object
   `u0`, on subdomain 0, i.e., the $x=1$ boundary;
 * a Robin condition (ref{ftut:poisson:multi:bc:Robin})
   on subdomain 1, $x=1$, with `Expression` or `Constant` objects
   `r` and `s` specifying $r$ and $s$;
 * a Neumann condition $\partial u/\partial n=g$ on subdomain 2, $y=0$,
   where an `Expression` or `Constant` object `g` implements the value $g$;
 * a homogeneous Neumann condition $\partial u/\partial n=0$ on
   subdomain 3, $y=1$.

As explained in Section ref{ftut:poisson:multiple:Dirichlet},
multiple Dirichlet conditions must be collected in a list of
`DirichletBC` objects. Based on the `boundary_conditions` data
structure above, we can construct this list by the following snippet:

@@@CODE-4 src/poisson_vc.py fromto: bcs = \[@if debug:

The new aspect of the variational problem is the two distinct
boundary integrals over $\Gamma_{N,i}$ and $\Gamma_{R,i}$.
Having a mesh function over exterior cell facets (our
`boundary_parts` object), where subdomains (boundary parts) are
numbered as $0,1,2,\ldots$, the special symbol `ds(0)`
implies integration over subdomain (part) 0, `ds(1)` denotes
integration over subdomain (part) 1, and so on.
The idea of multiple `ds`-type objects generalizes to volume
integrals too: `dx(0)`, `dx(1)`, etc., are used to
integrate over subdomain 0, 1, etc.,  inside $\Omega$.

Before we have `ds(n)` for integers `n` defined, we must do

!bc pycod
ds = Measure('ds', domain=mesh, subdomain_data=boundaries_parts)
!ec
Similarly, if we want integration of different parts of the domain,
we redefine `dx` as

!bc pycod
dx = Measure('dx', domain=mesh, subdomain_data=domains)
!ec
where `domains` is a `CellFunction` defining subdomains in $\Omega$.

Suppose we have a Robin condition with values `r` and `s` on subdomain
`R`, a Neumann condition with value `g` on subdomain `N`, the
variational form can be written

!bc pycod
a = dot(grad(u), grad(v))*dx + r*u*v*ds(R)
L = f*v*dx - g*v*ds(N) + r*s*v*ds(R)
!ec

In our case things get a bit more complicated since the
information about integrals in Neumann and Robin conditions
are in the `boundary_conditions` data structure. We can collect
all Neumann conditions by the code

@@@CODE-4 src/poisson_vc.py from-to: # Collect Neumann@# Collect Robin
Applying `sum(Nemann_integrals)` will apply the `+` operator to
the variational forms in the `Numeann_integrals` list and result
in the integrals we need for the right-hand side `L` of the
variational form.

The integrals in the Robin condition can similarly be collected
in lists:

@@@CODE-4 src/poisson_vc.py from-to: # Collect Robin@# Simpler Robin

We are now in a position to define the `a` and `L` expressions
in the variational formulation:

@@@CODE-4 src/poisson_vc.py from-to: # Define variational problem, solver_bc@# Simpler variational


=== Simplified handling of the variational formulation ===

We carefully ordered the terms in the variational formulation above
into the $a$ and $L$ parts. This requires a splitting of the Robin
condition and makes the `a` and `L` expressions less readable (still we
think understanding this splitting is key for any finite element programmer!).
Fortunately, UFL allow us to specify the complete variational form
(ref{ftut:poisson:multi:bc:varform}) as *one expression* and offer tools to
extract what goes into the bilinear form $a(u,v)$ and the linear form
$L(v)$:

!bc pycod
F = dot(p*grad(u), grad(v))*dx + \
    sum(Robin_integrals) - f*v*dx + sum(Neumann_integrals)
a, L = lhs(F), rhs(F)
!ec
This time we can more naturally define the integrals from the
Robin condition as `r*(u-s)*v*ds(n)`:

@@@CODE-4 src/poisson_vc.py from-to: # Simpler Robin@# Define variational problem, solver_bc

The complete code is in the `solver_bc` function in the
`${prog["poisson_vc"]}.py` file.


=== Test problem ===

Let us continue to use $\uex=1+x^2+2y^2$ as the exact solution, and
set $p=1$ and $f=-6$ in the PDE.  Our domain is the unit square, and
we assign Dirichlet conditions at $x=0$ and $x=1$, a Neumann condition
at $y=1$, and a Robin condition at $y=0$. With the given $\uex$, we
realize that the Neumann condition is $-4y$ (which means $-4$ at
$y=1$), while the Robin
condition can be selected in many ways. Since $\partial u/\partial
n=-\partial u/\partial y=0$ at $y=0$, we can select $s=u$ and have $r$
arbitrary in the Robin condition.

The boundary parts are $\Gamma_{D,0}$: $x=0$, $\Gamma_{D,1}$: $x=1$,
$\Gamma_{R,0}$: $y=0$, and $\Gamma_{N,0}$: $y=1$.

When implementing this test problem (and especially other test problems
with more complicated expressions), it is advantageous to use
symbolic computing. Below we define $\uex$ as a `sympy` expression
and derive other functions from their mathematical definitions.
Then we turn these expressions into C/C++ code, which can be
fed into `Expression` objects.

@@@CODE src/poisson_vc.py fromto: def application_bc_test\(@test_solvers_bc
This simple test problem is turned into a real unit test for different
function spaces in the function `test_solver_bc`.

=== Debugging the setting of boundary conditions ===

It is easy to make mistakes when implementing a problem with many
different types of boundary conditions, as in the present case. Some
helpful debugging output is to run through all vertex coordinates and
check if the `SubDomain.inside` method marks the vertex as on the
boundary. Another useful printout is to list which degrees of freedom
that are subject to Dirichlet conditions, and for first-order Lagrange
elements, add the corresponding vertex coordinate to the output.

@@@CODE-4 src/poisson_vc.py fromto: if debug:@# Collect Neumann
In addition, it is helpful to print the exact and the numerical solution
at all the vertices as shown in Section ref{ftut:poisson1:verify1}.

=== Implementation of multiple subdomains ===

Section ref{ftut:possion:2D:2mat:impl} explains how to deal with
multiple subdomains of $\Omega$ and a piecewise constant coefficient
function $p$ that takes on different constant values in the different
subdomains. We can easily add this type of $p$ coefficient to the
`solver_bc` function. The signature of the function is

!bc pycod
def solver_bc(
    p, f,                   # Coefficients in the PDE
    boundary_conditions,    # Dict of boundary conditions
    Nx, Ny,                 # Cell division of the domain
    degree=1,               # Polynomial degree
    subdomains=[],          # List of SubDomain objects in domain
    linear_solver='Krylov', # Alt: 'direct'
    abs_tol=1E-5,           # Absolute tolerance in Krylov solver
    rel_tol=1E-3,           # Relative tolerance in Krylov solver
    max_iter=1000,          # Max no of iterations in Krylov solver
    log_level=PROGRESS,     # Amount of solver output
    dump_parameters=False,  # Write out parameter database?
    debug=False,
    ):
...
    return u, p   # p may be modified
!ec
If `subdomain` is an empty list, we assume there are no subdomains, and
$p$ is an `Expression` or `Constant` object specifying a formula for
$p$. If not, `subdomain` is a list of `SubDomain` objects, defining
different parts of the domain. The first element is a dummy object,
defining ``the rest'' of the domain. The next elements define specific
geometries in the `inside` methods. We start by marking all elements
with subdomain number 0, this will then be ``the rest'' after marking
subdomains 1, 2, and so on. The next step is to define `p` as a
piecewise constant function over cells and fill it with values.
We assume that the user-argument `p` is an array (or list) holding
the values of $p$ in the different parts corresponding to `subdomains`.
The returned `p` is needed for flux computations. If there are no
subdomains, the returned `p` is just the original `p` argument.

The appropriate code for computing `p` becomes

!bc pycod
import numpy as np
if subdomains:
    # subdomains is list of SubDomain objects,
    # p is array of corresponding constant values of p
    # in each subdomain
    materials = CellFunction('size_t', mesh)
    materials.set_all(0)  # "the rest"
    for m, subdomain in enumerate(subdomains[1:], 1):
        subdomain.mark(materials, m)

    p_values = p
    V0 = FunctionSpace(mesh, 'DG', 0)
    p  = Function(V0)
    help = np.asarray(materials.array(), dtype=np.int32)
    p.vector()[:] = np.choose(help, p_values)
!ec

We define $p(x,y)=p_0$ in $\Omega_0$ and $k(x,y)=p_1$ in $\Omega_1$,
where $p_0>0$ and $p_1>0$ are given constants.
As boundary conditions, we choose $u=0$ at $y=0$, $u=1$ at $y=1$,
and $\partial u/\partial n=0$ at $x=0$ and $x=1$.
One can show that the exact solution is now given by

!bt
\begin{equation}
u(x, y) = \left\lbrace\begin{array}{ll}
{2yp_1\over p_0+p_1}, & y \leq 1/2\\
{(2y-1)p_0 + p_1\over p_0+p_1}, & y \geq 1/2
\end{array}\right.
\end{equation}
!et
As long as the element boundaries coincide with the internal boundary
$y=1/2$, this piecewise linear solution should be exactly recovered
by Lagrange elements of any degree. We can use this property to verify
the implementation and make a unit test for a series of function
spaces:

@@@CODE src/poisson_vc.py fromto: def test_solvers_bc_2mat@application_flow_around_circle

##=== Application: Porous media flow around a circular obstacle ===

##Should be done properly by Rectangle and Circle in mshr
##@@@CODE src/poisson_vc.py fromto: def application_flow_around_circle@if __name

##Don't show this without fancy ParaView plot.


##===== Speeding up SubDomain implementations by C++ strings =====

# Refactoring with classes? Separate section?
===== Refactoring of a solver function into solver and problem classes =====

A FEniCS solver for a PDE can be implemented in a general way, but
the problem-dependent data, like boundary conditions, must be specified
in each case by the user. The implementation in the previous section
required the user to supply a `boundary_conditions` dictionary with
specifications of the boundary condition on each of the four sides of
the unit square. If we, e.g., want two Dirichlet conditions at one
side, as our mathematical formulation of the problem in the previous
section in fact supports, this is not possible without extending
the `solver_bc` function.

A different software design is to introduce a problem class and
methods, supplied by the user from case to case, where boundary
conditions and other input data are defined. Such a design is used in
a lot of more advanced FEniCS application codes, and it is time to
exemplify it here.  As a counterpart to the solver function, we
introduce a solver class, but all the arguments for various input data
are instead method calls to an instance of a *problem class*. This
puts a somewhat greater burden on the programmer, but it allows for
more flexibility, and the code for, e.g., boundary conditions can be
more tailored to the problem at hand than the code we introduced in
the `solver_bc` function in the previous section.

The solver class will need problem information and for this purpose
call up the methods in a problem class. For example, the solver
gets the $f$ and $p$ functions in the PDE problem by calling
`problem.f_rhs()` and `problem.p_coeff()`. The mesh object and the
polynomial degree of the elements are supposed to be returned from
`problem.mesh_degree()`. Furthermore, the problem class defines the
boundary conditions in the problem as lists of minimal information
from which the solver can build proper data structures.

The solver class is a wrapping of the previous `solver_bc` and `flux`
functions as methods in a class, but some of the code for handling
boundary conditions in `solver_bc` is now delegated to the user in
the problem class.

@@@CODE src/poisson_class.py fromto: from fenics import@class Problem
Note that this is a general Poisson problem solver that works in any number
of space dimensions and with any mesh and composition of boundary conditions.

!bnotice Tip: Be careful with the `mesh` variable!
In classes, one often stores the mesh in `self.mesh`. When you need
the mesh, it is easy to write just `mesh`, but this gives rise to
peculiar error messages, since `mesh` is a Python module imported
by `from fenics import *` and already available as a name in your file.
When encountering strange error messages in statements containing a
variable `mesh`, make sure you use `self.mesh`.
!enotice

Below is the specific problem class for solving a scaled 2D Poisson
problem.  We have a two-material domain where a rectangle
$[0.3,0.7]\times [0.3,0.7]$ is embedded in the unit square and where
$p$ has a constant value inside the rectangle and another value
outside. On $x=0$ and $x=1$ we have homogeneous Neumann conditions,
and on $y=0$ and $y=1$ we have the Dirichlet conditions $u=1$ and
$u=0$, respectively.

@@@CODE src/poisson_class.py fromto: class Problem1@def demo

A specific problem can be solved by

@@@CODE src/poisson_class.py fromto: def demo@def test_Solver
The complete code is found in the file `${prog["poisson_class"]}.py`.

!bnotice Pros and cons of solver/problem classes vs solver function
What are the advantages of class `Solver` and `Problem` over the
function implementation in Section ref{ftut:poisson:multi:bc}?
The primary advantage is that
the class version works for any mesh and any composition of
boundary conditions, while the solver function is tied to a mesh
over the unit square, only one type of boundary condition on a
each side, and a piecewise constant $p$ function. The programmer has
to supply more code in the class version, but gets greater flexibility.
The disadvantage of the class version is that it applies the class
concept so one needs experience with Python class programming.
!enotice
